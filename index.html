<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>Hanbyul Joo | SNU CSE </title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:300,600' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="style.css">

	
<!-- Global site tag (gtag.js) - Google Analytics -->
<!--script async src="https://www.googletagmanager.com/gtag/js?id=UA-50740543-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50740543-1');
</script-->

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3H1937VV3Q"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3H1937VV3Q');
</script>

</head>


<body>
<table border="0" cellpadding="15" cellspacing="0" width="1000">
<tr>
<td>&nbsp;</td>
</tr>

<tr>
<td valign="top">
</td>
	<td valign="top">
		<table border="0" cellpadding="0" cellspacing="0" width="100%">
			<td align="left" width="40%">
			<!--img width="60" src="img/RI_logo.jpg"-->
			<p>
			
			<p><span class="Name"><a id="Home"></a><b>Hanbyul (Han) Joo</b></span><br> <br>
			Assistant Professor<br>
			<a href="https://cse.snu.ac.kr/en">Department of Computer Science and Engineering</a><br>
			<a href="https://en.snu.ac.kr/index.html">Seoul National University (SNU)</a><br>
			Email:  hbjoo-at-snu-ac-kr<br>
			Office: Rm 324 Bldg 302<br>
			<a href="https://scholar.google.com/citations?user=YMgdDBwAAAAJ&hl=en">[Google Scholar]</a>
			<a href="CV_Hanbyul.pdf">[CV]</a> <br><br>
			<img src="img/logo_snu_vclab.jpg" border=0 style="max-height: 90px">
			 <p>
		</td>
		<td  valign="bottom" align="left">
			<table>
			<tr>	
			<!--td> <img src="img/hbjoo.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> </td-->
			<td> <img src="img/han_dec_2017.jpg" border=0 style="max-height: 250px "></a> </td>
			<td> <img src="img/Dome_withME_s.jpg" border=0 style="max-height: 250px"></a> </td>

			</tr>
			</table>
		</td>
		</table>

<p>
<hr size="1" align="left" noshade>
<p>
I am an assistant professor at Seoul National University (SNU) in the Department of Computer Science and Engineering. Before joining SNU, I was a Research Scientist at Facebook AI Research (FAIR), Menlo Park. I finished my Ph.D. in the Robotics Institute at Carnegie Mellon University, where I worked with <a href="http://www.cs.cmu.edu/~yaser/">Yaser Sheikh</a>. During my PhD, I interned at Facebook Reality Labs, Pittsburgh (Summer and Fall, 2017) and Disney Research Zurich (Summer, 2015). I received my M.S. in EE and B.S. in CS at KAIST, Korea. I am a recipient of the <a href="http://slsf.or.kr/community/HomeEn.screen">Samsung Scholarship</a> and the <a href="https://www.thecvf.com/?page_id=413#CVPRBestStudent">CVPR Best Student Paper Award</a> in 2018. </p>
<br>

<b> <font color="blue">NEW</font> </b> (as of Jan 5, 2024) Full-time Research Engineer를 선발합니다. 관련 공고를 확인해주세요: <a href="https://docs.google.com/document/d/e/2PACX-1vS4F52vLGfXbCVy0AXCIO0IXPFLZ4-4rs4ShK4bRm-OCWeQ0dv1n2FP9VBdtBhl7_ELtsZej_z39DcH/pub">Link</a> <br>


<b> <font color="blue">NEW</font> </b> (as of Mar 15, 2024) I am hiring research intern students, who are interested in joining our lab for MS/PhD.
See more details <a href="https://docs.google.com/document/d/e/2PACX-1vSXVb0Fpq7trZ5wLMInr3vGEfp59rVlRM49L08JyvGF0vGsakE3Wo9-v3GzfUH-Ifl-geSPbb_9jUDz/pub">here</a>.




<!--b>Internship:</b> I am looking for graduate students to collaborate with me at FAIR. Feel free to contact me if you are interested in.-->

<h2>Research</h2>
<!--p>
My main research interests are in computer vision, computer graphics, and machine perception of social behavior. My work focuses on analyzing and understanding dynamic scenes using more than 500 synchronized cameras in the Panoptic Studio at CMU.
</p-->
<p>
<!--The goal of my research is to build the <b>"social Artificial Intelligence"</b> which can interact with humans using social signals (aka Body Languages). I pursue the direction using a data-driven method where the data is collected by measuring the wide spectrum of social signals transmitted during interpersonal social interaction. I developed a unique sensing system at CMU, the Panoptic Studio, which is composed of more than 500 synchronized cameras. My research is based on the tools of <b>computer vision, machine learning, computer graphics, and robotics.</b>--> 
The goal of my research is to endow machines and robots with the ability to perceive and understand human behaviors in 3D. Ultimately, I dream to build an AI system that can behave like humans in new environments and can interact with humans using a broad channel of nonverbal signals (kinesic signals or body languages). I pursue this direction by creating new tools in computer vision, machine learning, and computer graphics. 
</p>

<p align="center">

<table width="200" border="0" >
            <tbody>
				
              <tr>
				  <td><iframe width="360" height="202" src="https://www.youtube.com/embed/wb32z_xwk0c?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></td>
				  
                 <td>&nbsp;</td>
				 <td><iframe width="360" height="202" src="https://www.youtube.com/embed/HeXqiK0eGec?rel=0%3B3&amp%3Bautohide=1&amp%3Bshowinfo=0" frameborder="0" allowfullscreen></iframe></td>
				  
                <td>&nbsp;</td>
				  <!--td><iframe width="360" height="202" src="https://www.youtube.com/embed/yzAtedDYLrc" frameborder="0" allowfullscreen></iframe></td-->
				  <td><iframe width="360" height="202" src="https://www.youtube.com/embed/mTzwM9ovwSE?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
			  </tr>
              <tr align="center">
                <td><a href="http://domedb.perception.cs.cmu.edu/">The Panoptic Studio </a><br> </td>
                <td>&nbsp;</td>
				<td><a href="https://jlogkim.github.io/parahome/">SNU ParaHome</a><br> </td>
				<td>&nbsp;</td>
				<!--td><a href="http://domedb.perception.cs.cmu.edu/">Panoptic DB <br> </td-->
				<td><a href="https://jiyewise.github.io/projects/MocapEvery/">MocapEve&sup2</a><br> </td>
              </tr>
            </tbody>
</table>
		  
<!--iframe width="360" height="202" src="https://www.youtube.com/embed/Ryawcs6OZmI" frameborder="0" allowfullscreen></iframe-->
</p>

<!--p align="center">
<iframe width="560" height="315" src="//www.youtube.com/embed/xoPQnmkufeM?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
<h4 align="center"> Virtural Hanbyul in the Panoptic Studio</h4>
</p-->


<h2>News</h2>
<ul> 
<li> <p> <b> Jul 2024</b> I am co-organizing a workshop on <a href="https://sites.google.com/view/asi-eccv-2024/home">"Artificial Social Intelligence"</a> in conjunction with ECCV 2024</p></li>	
<li> <p> <b> Jul 2024</b> I will be giving a talk in the <a href="https://hands-workshop.org/">Observing and Understanding Hands in Action</a> workshop in conjunction with ECCV 2024</p></li>	
<li> <p> <b> Jul 2024</b> Our <a href="https://snuvclab.github.io/coma/">COMa</a> got accepted to ECCV 2024 as an oral publication</p></li>
<li> <p> <b> Jun 2024</b> I will be giving a talk in the <a href="https://egomotion-workshop.github.io/">EgoMotion</a> workshop in conjunction with CVPR 2024</p></li>
<li> <p> <b> Jun 2024</b> I am co-organizing a workshop on <a href="https://vto-cvpr24.github.io/">"Virtual Try-On"</a> in conjunction with CVPR 2024</p></li>
<li> <p> <b> May 2024</b> I serve as an area chair for NeurIPS 2024</p></li>
<li> <p> <b> May 2024</b> The demo set of ParaHome DB has been released: <a href="https://github.com/snuvclab/ParaHome">[Code/Dataset]</a>. More coming soon! </p></li>
<li> <p> <b> Apr 2024</b> I will be giving a talk in the RSS workshop on <a href="https://dex-manipulation.github.io/rss2024/">Dexterous Manipulation</a>: The talk video is available <a href="https://www.youtube.com/watch?v=DDRq0m80a9w&list=PLcFTgk4zQOSy0p-TQEEMXaGi918fmUxwA&index=9&t=10s">here</a> </p></li>
<li> <p> <b> Mar 2024</b> My students <a href="https://jiyewise.github.io/">Jiye</a> and <a href="https://bjkim95.github.io/">Byungjun</a> will start their internships in Codec Avatars Lab, Meta (Pittsburgh). Congrats! </p></li>
<li> <p> <b> Feb 2024</b> Four papers got accepted in CVPR 2024</p></li>
<li> <p> <b> Feb 2024</b> Congratulations to Yonwoo for earning an MS degree!</p></li>
<li> <p> <b> Jan 2024</b> We have introduced our new multi-camera system: <a href="https://jlogkim.github.io/parahome">SNU ParaHome</a></p></li>
<li> <p> <b> Jan 2024</b> I serve as an area chair for CVPR 2024</p></li>
<li> <p> <b> Jul 2023</b> Four papers got accepted in ICCV 2023 (including two oral publications)</p></li>
<li> <p> <b> May 2023</b> I am co-organizing a workshop on <a href="https://sites.google.com/view/asi-iccv-2023/home">"Artificial Social Intelligence"</a> in conjunction with ICCV 2023</p></li>
<li> <p> <b> May 2023</b> I serve as an area chair for CVPR 2023, ICCV 2023, NeurIPS 2023, WACV 2023</p></li>
<!-- <li> <p> <b> Jun 2022</b> I serve as a Technical Paper Committee for SIGGRAPH Asia 2022</p></li>
<li> <p> <b> Jun 2022</b> I am co-organizing a workshop on <a href="https://sites.google.com/berkeley.edu/artificial-social-intelligence">"Artificial Social Intelligence"</a> in conjunction with CVPR 2022</p></li>
<li> <p> <b> Mar 2022</b> Three papers got accepted to CVPR 2022</p></li>
<li> <p> <b> Feb 2022</b> I joined SNU as an assistant professor</p></li> -->
<!--<li> <p> <b> Dec 2021</b> Selected as one of outstanding reviewers in 3DV 2021</p></li>
<li> <p> <b> Feb 2021</b> Our <a href="http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/">Body2Hands</a> paper got accepted in CVPR 2021 </p></li>
<li> <p> <b> Oct 2020</b> One paper got accepted in NeurIPS 2020 </p></li>
<li> <p> <b> Aug 2020</b> We have released FrankMocap, an easy-to-use 3D body+hand pose estimator from images/videos: <a href="https://github.com/facebookresearch/frankmocap">Link</a> </p></li>
<a href="https://github.com/facebookresearch/frankmocap"><img style="max-height: 130px;" src="/img/eft_bodymocap.gif" border="0"></a>
<a href="https://github.com/facebookresearch/frankmocap"><img style="max-height: 130px;" src="/img/frankmocap_hand.gif" border="0"></a>

<li> <p> <b> July 2020</b> We have released EFT psuedo 3D human pose dataset: <a href="https://github.com/facebookresearch/eft">Link</a> </p></li>

<li> <p> <b> July 2020</b> One paper got accepted in ECCV 2020 </p></li>

<li> <p> <b> June 2020</b> Our PIFuHD went super viral! Check out fun results in twitter with #pifuhd hash tag: <a href="https://twitter.com/search?q=pifuhd&src=typed_query&f=live">Link</a> </p></li>

<li> <p> <b> June 2020</b> Better late than never, we have released the "processed version" of haggling dataset with annotations: <a href="https://github.com/CMU-Perceptual-Computing-Lab/ssp">Link</a> </p></li>

<li> <p> <b> June 2020</b> PIFuHD code has been released: <a href="https://github.com/facebookresearch/pifuhd">Link</a> </p></li>

<li> <p> <b> Feb 2020</b> Two papers got accepted as Oral Publications in CVPR 2020. </p></li> -->

<!--li> <p> <b> June 2019</b> Code and dataset of our <b>Monocular Total Capture (MTC)</b> paper has been released: <a href="http://domedb.perception.cs.cmu.edu/mtc">link</a> </p></li>

<li> <p> <b> Mar 2019</b> Two papers got accepted as Oral Publications in CVPR 2019. </p></li>

<li> <p> <b> Jan 2019</b> I joined Facebook AI Research (FAIR) as a research scientist from Jan 2019. </p></li>

<li> <p> <b> Dec 2018</b> I have defended my PhD thesis!!</li>

<li> <p> <b> Sep 2018</b> I talked about our research direction in the interview with <a href="https://twimlai.com/twiml-talk-180-can-we-train-an-ai-to-understand-body-language-with-hanbyul-joo/">"This Week in Machine Learning & AI"</a></p></li>

<li> <p> <b> Jun 2018</b> Our <a href="/totalcapture/">Total Capture paper</a> received <a href="http://cvpr2018.thecvf.com/program/main_conference#awards"><b>Best Student Paper Award</b> at CVPR 2018 <a></p></li>
<iframe width="360" height="202" src="https://www.youtube.com/embed/gz2VoDrvX-A?rel=0&amp;showinfo=0&amp;start=1023" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<a href="https://youtu.be/gz2VoDrvX-A?t=26m19s" target="_blank"><img style="max-height: 202px;" src="/img/cvpr_award.jpg" border="0"></a> </p></li>

<li> <b> <b> Feb 2018</b> Our research has been featured on <a href="https://www.wired.com/story/inside-the-panoptic-studio/">WIRED</a><!--: <br> &nbsp;&nbsp;
<a href="https://www.wired.com/story/inside-the-panoptic-studio/"><img style="max-height: 200px;" src="img/wired_han.jpg" border="0"></a> </p></li--->

<!-- <li> <p> <b> Feb 2018</b> I have two papers accepted to CVPR 2018 </p></li> -->


<!--li> <p> <b> Oct 2017</b> Our research has been featured on <a href="https://www.reuters.com/video/2017/09/22/500-camera-dome-trains-computer-to-read?videoId=372591598">Reuters</a> and <a href="http://www.bbc.com/news/av/technology-41584732/the-dome-which-could-help-machines-understand-behaviour">BBC News</a>
</p></li>

<li> <p> <b> Jul 2017</b>
Our <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose Library</a> and <a href="http://domedb.perception.cs.cmu.edu">Panoptic Studio</a> have been covered in multiple media outlets: <a href="http://spectrum.ieee.org/video/robotics/robotics-software/robots-learn-to-speak-body-language">IEEE Spectrum</a>, <a href="https://techcrunch.com/2017/07/07/cmu-researchers-create-a-huge-dome-that-can-read-body-language/">TechCrunch</a>, <a href="http://www.zdnet.com/article/keypoint-detection-unlocks-secrets-of-body-language/">ZDNet</a>, <a href="http://triblive.com/business/technology/12522553-74/at-cmus-panoptic-studio-531-cameras-capture-the-tiniest-details-of-human">Tribune-Review</a>, <a href="https://www.sciencedaily.com/releases/2017/07/170706143158.htm">ScienceDaily</a>, <a href="https://www.cs.cmu.edu/news/computer-reads-body-language">CMU News</a>, and so on. 

</p></li>

<li> <p> <b> Jul 2017</b> I am co-organizing a tutorial on <a href="http://domedb.perception.cs.cmu.edu/tutorials/cvpr17">"DIY: A Multiview Camera System: Panoptic Studio Teardown"</a> in conjunction with CVPR 2017
</p></li>

<li> <p> <b> Jul 2017</b> The code and realtime demo of our hand paper has been released as a part of <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">CMU OpenPose Library</a>
</p></li>

<li> <p> <b> May 2017</b> Since May 2017, I am interning at Oculus Research Pittsburgh.</p></li>

<li> <p> <b> May 2017</b> I have successfully finished my thesis proposal: <a href="http://ri.cmu.edu/event/hanbyul-joo-measuring-and-modeling-kinesic-signals-in-social-communication/">Thesis Info</a>
</p></li>

<li> <p> <b> April 2017</b> I was on a Korean TV channel (EBS), introducing our research: <a href="https://youtu.be/Axe_VkhBjPw?t=3m20s">Video</a>
</p></li-->

<!--li> <p> <b> Dec 2016</b> I gave talks in Bay Area (Stanford, UC Berkeley, and Adobe Research) to present our recent progress in social interaction capture using our Panoptic Studio. 
</p></li>


<li> <p> <b> Dec 2016</b> Our recent progress on the Panoptic Studio is featured on <a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox"><img src="/logos/Verge.png" height="20"></a>. You can also see the video version <a href="https://www.youtube.com/watch?v=Ryawcs6OZmI">here</a>. 
</p></li>


<li> <p> <b> Nov 2016</b> I gave a talk about our Panoptic Studio system in <a href="http://wwwhome.ewi.utwente.nl/~truongkp/icmi2016-assp4mi/">ASSP4MI</a> workshop of <a href="https://icmi.acm.org/2016/index.php?id=program">ICMI</a>. 
</p></li>


<li> <p> <b> Sep 2016</b> Our brand new <b>Panoptic Studio dataset website</b> is open <a href="http://domedb.perception.cs.cmu.edu">(link)</a>. All dataset including videos from 500+ cameras and 3D skeltons reconstruction results, and a Toolbox are available. 
</p></li>

<li> <p> <b> Dec 2015</b> Our Social Motion Capture paper has been covered in the press: <a href="http://www.spiegel.de/netzwelt/gadgets/das-panoptische-studio-computer-entziffern-koerpersprache-a-1068155.html">SPIEGEL ONLINE (German)</a> and <a href="http://www.fastcodesign.com/3054561/inside-a-robot-eyeball-science-will-decode-our-body-language">Co.DESIGN</a>.
</p></li>


<li> <p> <b> Jun-Oct 2015</b> I have finished my internship at Disney Research Zurich, and came back to CMU. 
</p></li>

<li> <p> <b> Sep 2015</b> Our "Social Motion Capture" paper has been accepted as an oral presentation in ICCV 2015.
</p></li>

<li> <p> <b> Apr 2015</b> Our massively multiple view system and dynamic 3D reconstruction has been featured on the <a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429">
<img src="http://www.cs.cmu.edu/~hanbyulj/logos/Reuters_logo.png" height="20"></div></a> and <a href="http://www.voanews.com/content/new-studio-yields-most-detailed-motion-capture-in-3d/2743347.html"><img src="http://www.cs.cmu.edu/~hanbyulj/logos/VOA_logo.png" height="17"></a>. Click the links to see the articles.

</p></li>

<li> <p> <b> Jan 2015</b> Our massively multiple view system and dynamic 3D reconstruction has been featured on <img src="http://img4.wikia.nocookie.net/__cb20091118171916/logopedia/images/4/49/Discovery_Channel_2009.png" height="17">, Daily Planet TV Show. You can see the segment in <a href="http://www.discovery.ca/dailyplanet"> Daily Planet website</a>. 


<li><p><b> Aug 2014</b> The dataset of our <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibility.html"> dynamic 3D reconstruction paper </a> is now available in <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibilityDataset.html">this webpage</a>.</p></li>

<li><p><b> July 2014</b> Our <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibility.html"> dynamic 3D reconstruction paper </a> is featured on

<a href="http://spectrum.ieee.org/tech-talk/computing/software/camerafilled-dome-recreates-full-3d-motion-scenes"><img src="/logos/IEEESpectrum.jpeg" height="17"></a>, 
 <a href="http://www.nbcnews.com/tech/innovation/camera-studded-dome-tracks-your-every-move-precision-n161541"><img src="/logos/nbc-news.png" height="15"></a>, 
<a href="http://news.discovery.com/tech/gear-and-gadgets/dome-of-500-cameras-creates-amazing-3-d-flicks-140724.htm"><img src="/logos/discovery.jpg" height="17"></a>, and so on. Check out the <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibility.html">project page</a>. </p> </li-->
</ul>



<h2>Students</h2>
<table cellspacing="15">
<tr>
	<td width="30%">
		<a href="https://jiyewise.github.io/">Jiye Lee</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://bjkim95.github.io/">Byungjun Kim</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://canoneod.github.io/">Jeonghwan Kim</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://hyunsoocha.github.io">Hyunsoo Cha</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://jlogkim.github.io/">Jisoo Kim</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://taeksuu.github.io">Taeksoo Kim</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://sulwon-0516.github.io/">Inhee Lee</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://bsw1907.notion.site/Sangwon-Beak-94077dec9e8c4821bc073a3cb0892407">Sangwon Beak</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://nagooon.github.io/">Jeonghyeon Na</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://www.sshowbiz.xyz/">Hyeonwoo Kim</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://leesuh1.github.io/">Seungho Lee</a> (MS/PhD)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://willi19.github.io/">Mingi Choi</a> (Intern)
	</td>
</tr>
<!-- <tr>
	<td width="30%">
		<a href="https://jellyheadandrew.github.io/">Sookwan Han</a> (Intern)
	</td>
</tr> -->
<tr>
	<td width="30%">
		<a href="https://rureadyo.github.io/">Sungjae Park</a> (Intern)
	</td>
</tr>
<tr>
	<td width="30%">
		Junyoung Lee (Intern)
	</td>
</tr>
<tr>
	<td width="30%">
		Giyoung Ju (Intern)
	</td>
</tr>
<tr>
	<td width="30%">
		Younggeol Cho (Intern)
	</td>
</tr>
<tr>
	<td width="30%">
		Sangmin Kim (RE)
	</td>
</tr>

<tr>
	<td width="30%">
		Taeseung Gil (RE)
	</td>
</tr>

</table>
<h2>Alumni</h2>
<table cellspacing="15">
<tr>
	<td width="30%">
		<a href="https://yc4ny.github.io/">Yonwoo Choi</a> (MS)
	</td>
</tr>
<tr>
	<td width="30%">
		<a href="https://jellyheadandrew.github.io/">Sookwan Han</a> (Intern)
	</td>
</tr>
<!--tr>
	<td width="30%">
		Joonki Min (RE)
	</td>
</tr-->
</table>
<h2>Dataset/Library</h2>
<table cellspacing="15">
<tr>
	<td width="30%">
	<a href="https://github.com/facebookresearch/frankmocap"><img style="max-width: 220px;" src="/img/frankmocap_intro.jpg" border="0"></a>
	</td>
	<td>
			<p> <a href="https://github.com/facebookresearch/frankmocap">FrankMocap</a>
	</td>
</tr>
<tr>
	<td width="30%">
	<a href="http://domedb.perception.cs.cmu.edu"><img style="max-width: 220px;" src="/img/ExampleResults.jpg" border="0"></a>
	</td>
	<td>
 		<p> <a href="http://domedb.perception.cs.cmu.edu">CMU Panoptic Studio Dataset</a>
	</td>
</tr>
<tr>
	<td width="30%">
	<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose"><img style="max-width: 220px;" src="/img/pose_face_hands.gif" border="0"></a>
	</td>
	<td>
 		<p> <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose Library</a>
	</td>
</tr>
</table>


<h2>Tutorial</h2>
<table cellspacing="15">
<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/dome_tutorial.jpg" border="0">
	</td>
	<td>
	<p> <b> DIY A Multiview Camera System: Panoptic Studio Teardown </b>
	<br>
	<b>Hanbyul Joo</b>, Tomas Simon, Hyun Soo Park, Shohei Nobuhara, and Yaser Sheikh
	<br>
	In Conjunction with <em>CVPR 2017 </em>  <br>
			<a href="http://domedb.perception.cs.cmu.edu/tutorials/cvpr17/index.html">[Tutorial Page]</a> <a href="https://www.youtube.com/watch?v=wmQF3XKvL8s">[Video]</a>
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 


</table>

<h2>Publications</h2>
<table cellspacing="15">
	
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="https://raw.githubusercontent.com/jhugestar/jhugestar.github.io/master/img/zeorshot_hoi.jpg" border="0">
		</td>

		<td>
			<b> Beyond the Contact: Discovering Comprehensive Affordance for 3D Objects from Pre-trained 2D Diffusion Models</b>
			<br>
			<a href="https://www.sshowbiz.xyz/">Hyeonwoo Kim</a>*, <a href="https://jellyheadandrew.github.io/">Sookwan Han</a>*, Patrick Kwon, <b>Hanbyul Joo</b>
			<br>
			In <em> ECCV 2024 </em> <b> <font color="blue">(Oral)</font></b> <br>
			<!-- In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b> <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br> -->
			<a href="https://arxiv.org/pdf/2401.12978">[Paper]</a>
			<a href="https://snuvclab.github.io/coma/">[Project Page]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px; " src="https://blog.sulwon.com/img/gtu_v0_re2.jpg" border="0">
		</td>
		<td>
			<b> Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses
			</b>
			<br>
			<a href="https://sulwon-0516.github.io/">Inhee Lee</a>, <a href="https://bjkim95.github.io/">Byungjun Kim</a>, <b>Hanbyul Joo</b>
			<br>
			In <em> CVPR 2024 </em><br>
			<!-- In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b> <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br> -->
			<a href="https://arxiv.org/abs/2404.14410">[Paper]</a>
			<a href="https://snuvclab.github.io/gtu/">[Project Page]</a>
			<a href="https://github.com/snuvclab/gtu/">[Code]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px; " src="https://hyunsoocha.github.io/images/pegasus.jpeg" border="0">
		</td>
		<td>
			<b> PEGASUS: Personalized Generative 3D Avatars with Composable Attributes
			</b>
			<br>
			<a href="https://hyunsoocha.github.io">Hyunsoo Cha</a>, <a href="https://bjkim95.github.io/">Byungjun Kim</a>, <b>Hanbyul Joo</b>
			<br>
			In <em> CVPR 2024 </em><br>
			<!-- In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b> <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br> -->
			<a href="https://arxiv.org/abs/2402.10636">[Paper]</a>
			<a href="https://snuvclab.github.io/pegasus/">[Project Page]</a>
			<a href="https://github.com/snuvclab/pegasus/">[Code]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px; " src="https://snuvclab.github.io/gala/static/images/teaser_arxiv.png" border="0">
		</td>
		<td>
			<b> GALA: Generating Animatable Layered Assets from a Single Scan</b>
			<br>
			<a href="https://taeksuu.github.io/">Taeksoo Kim</a>*, <a href="https://bjkim95.github.io/">Byungjun Kim</a>*, <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a>, <b>Hanbyul Joo</b>
			<br>
			In <em> CVPR 2024 </em><br>
			<!-- In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b> <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br> -->
			<a href="https://arxiv.org/abs/2401.12979">[Paper]</a>
			<a href="https://snuvclab.github.io/gala/">[Project Page]</a>
			<a href="https://github.com/snuvclab/GALA">[Code]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="https://jiyewise.github.io/images/mocap_every_teaser.gif" border="0">
		</td>

		<td>
			<b> Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera</b>
			<br>
			<a href="https://jiyewise.github.io/">Jiye Lee</a>, <b>Hanbyul Joo</b>
			<br>
			In <em> CVPR 2024 </em><br>
			<!-- In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b> <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br> -->
			<a href="https://arxiv.org/abs/2401.00847">[Paper]</a>
			<a href="https://jiyewise.github.io/projects/MocapEvery/">[Project Page]</a>
			<a href="https://jiyewise.github.io/projects/MocapEvery/">[Code]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px; " src="https://raw.githubusercontent.com/jlogkim/parahome/master/static/images/teaser_part_resized.jpg" border="0">
		</td>

		<td>
			<b> ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions</b>
			<br>
			<a href="https://canoneod.github.io/">Jeonghwan Kim</a>*, <a href="https://jlogkim.github.io/">Jisoo Kim</a>*, <a href="https://nagooon.github.io/">Jeonghyeon Na</a>, <b>Hanbyul Joo</b>
			<br>
			<!-- In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b> <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br> -->
			<a href="https://arxiv.org/pdf/2401.10232.pdf">[Paper]</a>
			<a href="https://jlogkim.github.io/parahome/">[Project Page]</a>
			<a href="https://github.com/snuvclab/ParaHome">[Code/Dataset]</a>
		</td>
	</tr>




	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="/img/chorus_mid.gif" border="0">
		</td>

		<td>
			<b> CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images</b>
			<br>
			<a href="https://jellyheadandrew.github.io/">Sookwan Han</a>, <b>Hanbyul Joo</b>
			<br>
			In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b> <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br>
			<a href="https://jellyheadandrew.github.io/projects/chorus/static/pdfs/paper.pdf">[Paper]</a>
			<a href="https://jellyheadandrew.github.io/projects/chorus/">[Project Page]</a>
			<a href="https://github.com/jellyheadandrew/CHORUS">[Code]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="https://bjkim95.github.io/images/Chupa.png" border="0">
		</td>

		<td>
			<b> Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models</b>
			<br>
			<a href="https://bjkim95.github.io/">Byungjun Kim</a>*, Patrick Kwon*, Kwangho Lee, Myunggi Lee, <a href="https://jellyheadandrew.github.io/">Sookwan Han</a>, Daesik Kim, <b>Hanbyul Joo</b>
			<br>
			In <em> ICCV 2023 </em> <b> <font color="blue">(Oral)</font></b>  <font class="ratio"> - Acceptance ratio: 152/8260 = 1.8% </font>  <br>
			<a href="https://arxiv.org/pdf/2305.11870.pdf">[arxiv]</a>
			<a href="https://snuvclab.github.io/chupa/">[Project Page]</a>
			<a href="https://github.com/snuvclab/chupa">[Code]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="https://taeksuu.github.io/ncho/static/images/teaser.png" border="0">
		</td>

		<td>
			<b> NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</b>
			<br>
			<a href="https://taeksuu.github.io/">Taeksoo Kim</a>, <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a>, <b>Hanbyul Joo</b>
			<br>
			In <em> ICCV 2023 </em><br>
			<a href="https://arxiv.org/abs/2305.14345">[arxiv]</a>
			<a href="https://taeksuu.github.io/ncho/">[Project Page]</a>
			<a href="https://github.com/taeksuu/ncho">[Code]</a>
		</td>
	</tr>
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="https://jiyewise.github.io/images/lama-teaser2.png" border="0">
		</td>

		<td>
			<b> Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments</b>
			<br>
			<a href="https://jiyewise.github.io/">Jiye Lee</a>, <b>Hanbyul Joo</b>
			<br>
			In <em> ICCV 2023 </em><br>
			<a href="https://arxiv.org/abs/2301.02667">[arxiv]</a>
			<a href="https://jiyewise.github.io/projects/LAMA/">[Project Page]</a>
			<a href="https://github.com/jiyewise/LAMA">[Code]</a>
		</td>
	</tr>




	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="https://gengshan-y.github.io/images/banmo.gif" border="0">
		</td>

		<td>
			<b> BANMo: Building Animatable 3D Neural Models from Many Casual Videos</b>
			<br>
			<a href="https://gengshan-y.github.io/">Gengshan Yang</a>, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, <b>Hanbyul Joo</b>
			<br>
			In <em> CVPR 2022 <b> <font color="blue">(Oral)</font> </b> </em>  <font class="ratio"> - Acceptance ratio: 344/8161 = 4.2% </font> <br>
			<a href="https://arxiv.org/abs/2112.12761">[arxiv]</a>
			<a href="https://banmo-www.github.io/">[Project Page]</a>
		</td>
	</tr>

	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="https://ego4d-data.org/assets/images/massive.png" border="0">
		</td>

		<td>
			<b> Ego4D: Around the World in 3,000 Hours of Egocentric Video</b>
			<br>
			Grauman et al.
			<br>
			In <em> CVPR 2022 <b> <font color="blue">(Oral)</font> </b> </em><font class="ratio"> - Acceptance ratio: 344/8161 = 4.2% </font><br>
			<b><font color="blue">[Best Paper Award Finalist]</font></b><br>
			<a href="https://arxiv.org/abs/2110.07058">[arxiv]</a>
			<a href="https://ego4d-data.org/">[Project Page]</a>
		</td>
	</tr>

	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src=img/ltl_cvpr22.jpg border="0">
		</td>

		<td>
			<b> Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion</b>
			<br>
			<a href="http://people.eecs.berkeley.edu/~evonne_ng">Evonne Ng </a>, <b>Hanbyul Joo</b>, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, Shiry Ginosar
			<br>
			In <em> CVPR 2022 </em><br>
			<a href="https://arxiv.org/abs/2204.08451">[arxiv]</a>
			<a href="https://evonneng.github.io/learning2listen/">[Project Page]</a>
			<a href="https://github.com/evonneng/learning2listen">[Code+Data]</a>
		</td>
	</tr>

	
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="img/21_human_intention.jpg" border="0">
		</td>

		<td>
			<b> Modeling Human Intention Inference in Continuous 3D Domains by Inverse Planning and Body Kinematics</b>
			<br>
			Yingdong Qian, Marta Kryven, <a href="https://www.psych.ucla.edu/faculty-page/taogao/">Tao Gao</a>, <b>Hanbyul Joo</b>, <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>
			<br>
			<em> arXiv 2021 </em><br>
			<a href="https://arxiv.org/abs/2112.00903">[arxiv]</a>
		</td>
	</tr>





	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="img/d3dhoi.png" border="0">
		</td>

		<td>
			<b> D3D-HOI: Dynamic 3D Human-Object Interactions from Videos</b>
			<br>
			<a href="https://samxuxiang.github.io/">Xiang Xu</a>, <b>Hanbyul Joo</b>, Greg Mori, Manolis Savva
			
			<br>
			<em> arXiv 2021 </em><br>
			<a href="https://arxiv.org/abs/2108.08420">[arxiv]</a>
			<a href="https://github.com/facebookresearch/d3d-hoi">[Code/Data]</a>
		</td>
	</tr>

		
	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="img/frankmocap.jpg" border="0">
		</td>

		<td>
			<b> FrankMocap: A Fast Monocular 3D Hand and Body Motion Capture by Regression and Integration</b>
			<br>
			<a href="https://penincillin.github.io/">Yu Rong</a>, Takaaki Shiratori, <b>Hanbyul Joo</b>
			<br>
			In <em> ICCV 2021 Workshop </em><br>
			<a href="https://arxiv.org/pdf/2008.08324.pdf">[arxiv]</a>
			<a href="https://penincillin.github.io/frank_mocap">[Project Page]</a>
			<a href="https://github.com/facebookresearch/frankmocap">[Code]</a>
		</td>
	</tr>

	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="img/eft.jpg" border="0">
		</td>

		<td>
			<b> Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation</b>
			<br>
			<b>Hanbyul Joo</b>, Natalia Neverova, Andrea Vedaldi
			<br>
			<em> 3DV 2021 <b> <font color="blue">(Oral)</font> </b> </em><br>
			<a href="https://arxiv.org/abs/2004.03686">[arxiv]</a> 
			<a href="https://github.com/facebookresearch/eft">[Code/Dataset]</a> 
		</td>
	</tr>


	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="img/body2hands.jpg" border="0">
		</td>

		<td>
			<b> Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics</b>
			<br>
			<a href="http://people.eecs.berkeley.edu/~evonne_ng">Evonne Ng</a>, <a href="https://people.eecs.berkeley.edu/~shiry/">Shiry Ginosar</a>, Trevor Darrell, <b>Hanbyul Joo</b>
			<br>
			In <em> CVPR 2021 </em><br>
			<a href="https://arxiv.org/abs/2007.12287">[arxiv]</a>
			<a href="http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/">[Project Page]</a>
		</td>
	</tr>

	<tr>
		<td width="30%">
		<img style="max-height: 300px; max-width: 250px;" src="img/3dmultibody.jpg" border="0">
		</td>
	
		<td>
			<b> 3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous Image Data</b>
			<br>
			<a href="http://mi.eng.cam.ac.uk/~bjb56/">Benjamin Biggs</a>, Seb Ehrhadt, <b>Hanbyul Joo</b>, Ben Graham, Andrea Vedaldi, David Novotny
			<br>
			In <em> NeurIPS 2020 </em> &nbsp <b> <font color="blue">(Spotlight)</font> </b> <br>
			<a href="https://arxiv.org/abs/2011.00980">[arxiv]</a>
			<a href="https://sites.google.com/view/3dmb/home">[Project Page]</a>
		</td>
	</tr>



<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/phosa.jpg" border="0">
	</td>

	<td>
		<b> Perceiving 3D Human-Object Spatial Arrangements
			from a Single Image in the Wild</b>
		<br>
		<a href="https://jasonyzhang.com/">Jason Y. Zhang</a>*, Sam Pepose*, <b>Hanbyul Joo</b>, Deva Ramanan, Jitendra Malik,  Angjoo Kanazawa
		<br>
		In <em> ECCV 2020 </em><br>
		<a href="https://arxiv.org/abs/2007.15649">[arxiv]</a>
		<a href="https://jasonyzhang.com/phosa/">[Project Page]</a>
	</td>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/cvpr2020_pifuhd.png" border="0">
	</td>

	<td>
		<b> PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</b>
		<br>
		<a href="http://www-scf.usc.edu/~saitos/">Shunsuke Saito</a>, Tomas Simon, Jason Saragih, <b>Hanbyul Joo</b>
		<br>
		In <em> CVPR 2020</em> &nbsp <b> <font color="blue">(Oral)</font> </b> <br>
		<a href="https://arxiv.org/pdf/2004.00452.pdf">[arxiv]</a> 
		<a href="https://shunsukesaito.github.io/PIFuHD">[Project Page]</a> 
		<a href="https://github.com/facebookresearch/pifuhd">[Code]</a> 
		<a href="https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt?usp=sharing">[Colab Demo]</a> 
	</td>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/cvpr2020_you2me.jpg" border="0">
	</td>

	<td>
			<b> You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions</b>
			<br>
			<a href="http://people.eecs.berkeley.edu/~evonne_ng">Evonne Ng</a>, Donglai Xiang, <b>Hanbyul Joo</b>, Kristen Grauman
			<br>
			In <em> CVPR 2020</em>  &nbsp <b> <font color="blue">(Oral)</font> </b> <br> <!----b> (Oral) </b> <font class="ratio"> - Acceptance ratio: 70/3303~2.1% </font> <br-->
			<a href="https://arxiv.org/abs/1904.09882">[arxiv]</a>
			<a href="http://vision.cs.utexas.edu/projects/you2me/">[Project Page]</a>
			<a href="https://github.com/facebookresearch/you2me#">[Code/Data]</a> 
	</td>
</tr>


<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="https://cdn-cms.f-static.com/uploads/399791/2000_5d92a4eecb301.png" border="0">
	</td>

	<td>
	<b> Single-Network Whole-Body Pose Estimation</b>
	<br>
	Gines Hidalgo, Yaadhav Raaj, Haroon Idrees, Donglai Xiang, <b>Hanbyul Joo</b>, Tomas Simon, Yaser Sheikh
	<br>
	In <em> ICCV 2019</em> <br> <!----b> (Oral) </b> <font class="ratio"> - Acceptance ratio: 70/3303~2.1% </font> <br-->
	<a href="https://arxiv.org/abs/1909.13423">[arxiv]</a>
	<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_train">[OpenPose Training]</a>
	<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">[OpenPose]</a>
	</td>
</tr>


<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/cvpr19_ssp.jpg" border="0">
	</td>
	<td>
	<b> Towards Social Artificial Intelligence: Nonverbal Social Signal Prediction in A Triadic Interaction</b>
	<br>
	<b>Hanbyul Joo</b>, Tomas Simon, Mina Cikara, Yaser Sheikh
	<br>
	In <em> CVPR 2019</em>  &nbsp <b> <font color="blue">(Oral)</font> </b> <br> <!----b> (Oral) </b> <font class="ratio"> - Acceptance ratio: 70/3303~2.1% </font> <br-->
	<a href="https://arxiv.org/pdf/1906.04158.pdf">[arxiv]</a>
	<a href="https://github.com/CMU-Perceptual-Computing-Lab/ssp">[Code/Dataset]</a>
	<a href="https://www.youtube.com/watch?v=cisBGgT0qY0">[Video]</a>
	</td>
</tr>


<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/mtc.jpg" border="0">
	</td>

	<td>
	<b> Monocular Total Capture: Posing Face, Body and Hands in the Wild </b>
	<br>
	<a href="https://xiangdonglai.github.io">Donglai Xiang</a>, <b>Hanbyul Joo</b>, Yaser Sheikh
	<br>
	In <em> CVPR 2019</em>  &nbsp <b> <font color="blue">(Oral)</font> </b> <br>  <!----b> (Oral) </b> <font class="ratio"> - Acceptance ratio: 70/3303~2.1% </font> <br-->
	<a href="https://arxiv.org/abs/1812.01598">[arxiv]</a>
		<a href="http://domedb.perception.cs.cmu.edu/mtc.html">[Dataset and Code]</a>
		<a href="https://www.youtube.com/watch?v=rZn15BRf77E">[Video]</a>
	</td>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/thesis_img.jpg" border="0">
	</td>

	<td>
	<b> Sensing, Measuring, and Modeling Social Signals in Nonverbal Communication </b>
	<br>
	<b>Hanbyul Joo</b>
	<br>
	PhD Thesis, Robotics Institute, Carnegie Mellon University, 2019 <br>
	<a href="https://www.dropbox.com/s/rknji031x45yi1q/thesis_final_submission.pdf?dl=0">[PDF]</a>
	</td>
</tr>


<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/totalcapture.gif" border="0">
	</td>

	<td>
	<b> Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies </b>
	<br>
	<b>Hanbyul Joo</b>, Tomas Simon, and Yaser Sheikh
	<br>
	In <em> CVPR 2018</em> &nbsp  <b> <font color="blue">(Oral)</font></b> <!--- Acceptance ratio: 70/3303~2.1% </font> <br-->
	&nbsp<a href="https://www.thecvf.com/?page_id=413#CVPRBestStudent"><font color="blue"> <br>
	<b>[CVPR Best Student Paper Award]</b></font></a> 
	<br>
	<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Joo_Total_Capture_A_CVPR_2018_paper.pdf ">[PDF]</a>
	<a href="/totalcapture/totalBody_camready_supp.pdf">[Supplementary Material]</a>
	<a href="https://www.youtube.com/watch?v=5QzdXQSf-oY">[video]</a>
	<a href="/totalcapture">[Project Page]</a>
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 


<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/cvpr18_skate.jpg" border="0">
	</td>

	<td>
	<b> Structure from Recurrent Motion: From Rigidity to Recurrency</b>
	<br>Xiu Li, Hongdong Li, <b>Hanbyul Joo</b>, Yebin Liu, Yaser Sheikh 
	<br>
	In <em> CVPR 2018</em> <br>
	<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Structure_From_Recurrent_CVPR_2018_paper.pdf">[PDF]</a>
	<!--a href="https://arxiv.org/abs/1801.01615">[arXiv]</a>
	<a href="https://www.youtube.com/watch?v=5QzdXQSf-oY">[video]</a>
	<a href="http://www.cs.cmu.edu/~hanbyulj/totalcapture">[Project Page]</a-->
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 



<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="img/teaser_mvbs.jpg" border="0">
	</td>
	<td>
	<b> Hand Keypoint Detection in Single Images using Multiview Bootstrapping </b>
	<br>
	<a href="http://www.cs.cmu.edu/~tsimon">Tomas Simon</a>, <b>Hanbyul Joo</b>, Iain Mattews, and Yaser Sheikh
	<br>
	In <em>CVPR 2017 </em>  <br>
	<a href="http://arxiv.org/abs/1704.07809">[arXiv]</a> <a href="http://www.cs.cmu.edu/~tsimon/projects/mvbs.html">[Project Page]</a>
	<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">[Code]</a>
	<a href="http://domedb.perception.cs.cmu.edu/handdb.html">[Dataset]</a>
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 


<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/panoptic.jpg" border="0">
	</td>
	<td>
 	<b>  Panoptic Studio: A Massively Multiview System for Social Interaction Capture </b>
	<br>
	<b>Hanbyul Joo</b>, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh
	<br>
	In <em> TPAMI 2017 (Extended version of ICCV15)</em>  <br>

	<a href="http://ieeexplore.ieee.org/document/8187699">[published version]</a>
	<a href="https://arxiv.org/abs/1612.03153">[arXiv version]</a>
	<a href="http://domedb.perception.cs.cmu.edu">[Dataset]</a> <br>
	</td>
</tr> 

<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="img/007_bang.gif" border="0">
	</td>
	<td>
 	<b>  Panoptic Studio: A Massively Multiview System for Social Motion Capture </b>
	<br>
	<b>Hanbyul Joo</b>, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara,
	and Yaser Sheikh
	<br>
	In <em>ICCV 2015</em> &nbsp <b> <font color="blue">(Oral)</font></b> <font class="ratio"> <!-- Acceptance ratio: 56/1698~3.3% </font -->
 
	<br>

	<a href="/panoptic-studio/ICCV2015_SMC.pdf"> [Paper(PDF)]</a> 
	<a href="/panoptic-studio/ICCV2015_SMC_Supp.pdf"> [Supplementary Material]</a> 

	<a href="/panoptic-studio">[Project Page]</a>  <br>
	Press Coverage: 


<a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox"><img src="/logos/Verge.png" height="20"></a>
<a href="https://www.technologyreview.com/s/601276/5-things-you-need-to-know-about-facebooks-next-10-years/"><img src="https://d267cvn3rvuq91.cloudfront.net/_/img/stacked-logo.svg?v=2" height="30"></a>
<!--a href="http://www.fastcodesign.com/3054561/inside-a-robot-eyeball-science-will-decode-our-body-language"><img src="https://pbs.twimg.com/profile_images/487710482539630592/ZuFzG-w6.png" height="40"></a-->
<a href="http://www.spiegel.de/netzwelt/gadgets/das-panoptische-studio-computer-entziffern-koerpersprache-a-1068155.html"><img src="/logos/spiegelOnline.jpg" height="15"></a>


</p>
	</td>
</tr>  


<tr>
	<td width="30%">
	<!--img style="max-height: 200px; max-width: 250px;" src="img/CVPR14.jpg" border="0" -->
	<img style="max-height: 200px; max-width: 250px;" src="img/confetti.gif" border="0">
	</td>
	<td>
<b>MAP Visibility Estimation for Large-Scale Dynamic 3D Reconstruction</b>
	<br>
	<b>Hanbyul Joo</b>, Hyun Soo Park, and Yaser Sheikh
	<br>
	In <em>CVPR 2014</em> &nbsp <b>(<font color="blue">(Oral)</font></b> <font class="ratio"> <!-- - Acceptance ratio: 104/1807~5.8% </font-->
	<br>

	<a href="/14/CVPR_2014_Visibility.pdf"> [Paper(PDF)]</a> 
	<a href="/14/visibility.html"> [Project Page]</a> <a href="/14/visibilityDataset.html">[Dataset]</a> <br>
	Press Coverage: 
<a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429"><img src="/logos/Reuters_logo.png" height="20"></div></a>
<a href="http://www.voanews.com/content/new-studio-yields-most-detailed-motion-capture-in-3d/2743347.html"><img src="/logos/VOA_logo.png" height="17"></a>
<a href="http://spectrum.ieee.org/tech-talk/computing/software/camerafilled-dome-recreates-full-3d-motion-scenes"><img src="/logos/IEEESpectrum.jpeg" height="20"></a>
<a href="http://www.cnet.com/videos/tomorrow-daily-021-new-video-capture-tech-the-rickroll-rickmote-episode-vii-new-x-wing/"><img src="http://upload.wikimedia.org/wikipedia/en/8/8f/Cnetlogo.png" height="30"></a>
<a href="http://www.nbcnews.com/tech/innovation/camera-studded-dome-tracks-your-every-move-precision-n161541"><img src="/logos/nbc-news.png" height="15"></a>
<a href="http://news.discovery.com/tech/gear-and-gadgets/dome-of-500-cameras-creates-amazing-3-d-flicks-140724.htm"><img src="/logos/discovery.jpg" height="20"></a>
<a href="http://www.engadget.com/2014/07/21/carnegie-motion-capture-dome/"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Engadget-logo.svg/200px-Engadget-logo.svg.png" height="20"></a>
<a href="http://gizmodo.com/a-dome-packed-with-480-cameras-captures-detailed-3d-ima-1608263411"><img src="http://i.kinja-img.com/gawker-media/image/upload/s--NkOZEtU4--/xxtpkkwu6xvbcaoxjbx2.png" height="13"><a>
<a href="http://www.theverge.com/2014/7/21/5923767/panoptic-studio-3d-motion-capture-carnegie-melon"><img src="/logos/Verge.png" height="20"></a>
<a href="http://www.sciencedaily.com/releases/2014/07/140717124950.htm"><img src="https://www.sciencedaily.com/images/sd-logo.png" height="15"></a>
<a href="http://phys.org/news/2014-07-combinehundreds-videos-reconstruct-3d-motion.html"><img src="/logos/phyOrg.jpg" height="25"></a>
<a href="http://www.slate.com/articles/video/video/2014/07/panoptic_studio_carnegie_mellon_s_3_d_camera_dome_can_capture_action.html"><img src="http://upload.wikimedia.org/wikipedia/commons/1/13/Slate_logo.png" height="22"></a>
<a href="http://petapixel.com/2014/07/21/researchers-use-480-camera-setup-accurately-capture-3d-motion/"><img src="/logos/petapixel.jpg" height="25"></a>
<a href="http://www.gizmag.com/3d-motion-reconstruction-camera-panoptic-dome/33033/"><img src="/logos/gizmag.jpeg" height="25"></a>
<a href="http://www.theregister.co.uk/2014/07/22/boffins_fill_a_dome_with_480_cameras_for_3d_motion_capture/"><img src="/logos/TheRegisterLogo.jpg" height="20"></a>
<a href="http://www.theengineer.co.uk/electronics/news/3d-motion-captured-without-markers/1018947.article"><img src="/logos/theEngineer.png" height="18"></a>...

<!--a href="http://www.popphoto.com/news/2014/07/carnegie-mellon-packs-480-cameras-dome-to-perfectly-track-3d-motion"><img src="/logos/PopPhoto.jpg" height="20"></a>
<a href="http://www.cmu.edu/news/stories/archives/2014/july/july17_reconstructing3Dmotion.html"><img src="/logos/CMU.jpg" height="40"></a> 
<a href="http://www.eurekalert.org/pub_releases/2014-07/cmu-cmc071714.php"><img src="/logos/EurekAlert.jpg" height="22"></a>

<a href="http://www.azorobotics.com/news.aspx?newsID=5985"><img src="/logos/AzoRobot.jpg" height="20"></a>

<a href="http://www.slrlounge.com/latest-3d-tech-geodome-captures-every-move-480-cameras"><img src="/logos/slrLounge.jpg" height="20"></a>

<a href="http://security-today.com/articles/2014/07/23/panopticon-captures-every-movement-in-3d.aspx"><img src="/logos/securityToday.jpg" height="20"></a-->

	</td>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="img/icip11_2.jpg" border="0">
	</td>
	<td>
<b>Graph-based Shape Matching for Deformable Objects</b>
	<br>
	<b>Hanbyul Joo</b>,
	Yekeun Jeong, Olivier Duchenne, and In So Kweon
	<br>
	In <em> ICIP 2011</em> 
	<br>
		<a href="/papers/ICIP2011_JOO_FINAL.pdf">
	[Paper(PDF)]
	</a> </p>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/icra09.jpg" border="0">
	</td>
	<td>
	<p><b>Graph-Based Robust Shape Matching for Robotic Application</b>
	<br>
	<b>Hanbyul Joo</b>,
	Yekeun Jeong, Olivier Duchenne, Seong-Young Ko, and In So Kweon
	<br>
	In <em>ICRA 2009</em> 
	<br>
	<a href="/papers/ICRA2009.pdf">
	[Paper(PDF)]
	</a> </p>
	</td>
</tr>
<!--tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/MVA07.jpg" border="0">
	</td>
	<td>
<b>Statistical Background Subtraction Based on the Exact Per-pixel Distributions</b>
	<br>
	Youngbae Hwang, <b>Hanbyul Joo</b>, Jun-sik Kim, and In So Kweon
	<br>
	In <em>IAPR workshop on Machine Vision Applications (MVA) 2007</em> 
	<br>
	<a href="
http://rcv.kaist.ac.kr/~unicorn/paper/hwang_MVA_2007.pdf">
	[Paper(PDF)]
	</a></p>
	</td>
</tr-->


</table>
 
<!--img 
src="http://monster.gostats.com/bin/count?a=400436&amp;t=4&amp;i=19&amp;z=" 
style="border-width:0px" alt="Free web counters" /><br / 
<small>since Jan 2007. </small -->
<!-- End GoStats.com Simple HTML based code -->
<h2>Talks</h2>
<ul>
<li> <p> <b>"Measuring and Modeling Human Motion"</b></p></li>
<ul>
	<li> Facebook AI Video Summit, June 2019.</li>
</ul>		
<li> <p> <b>"Towards Social Artificial Intelligence: Nonverbal Social Signal Prediction in A Triadic Interaction"</b></p></li>
<ul>
	<li> CVPR Oral Talk, June 2019 <a href="https://youtu.be/fNlMGWm7bbk?t=2536">(video link)</a>.</li>
</ul>
<li> <p> <b>"Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies"</b></p></li>
	<ul>
		<li> <a href="https://games-cn.org/games-webinar-20181025-70/">GAMES Webinar</a>, Oct 2018 (hosted by Prof. Yebin Liu).</li>
		<li> CVPR Oral Talk, June 2018 <a href="https://youtu.be/XWr0Fg5XbPs?t=268">(video link)</a>.</li>
	</ul>
<li> <p> <b>"Measuring and Modeling Social Signals for Computational Behavioral Understanding"</b></p></li>
	<ul>
		<li> UC Berkeley, BAIR, MAY 2018 (hosted by Prof. Jitendra Malik).</li>
		<li> UT Austin, School of Computer Science, April 2018.</li>
		<li> CMU, School of Computer Science, April 2018.</li>
		<li> MIT, CSAIL, April 2018.</li>
		<li> MIT, Media Lab, Nov 2017.</li>
	</ul>
<li> <p> <b>"The Panoptic Studio: A Massively Multiview System for Social Interaction Capture"</b></p></li>
	<ul>

		<li> UC Berkeley, Computer Vision Group, Dec 2016 (hosted by Prof. Alexei A. Efros).</li>
		<li> Stanford, Computer Vision and Geometry Lab (CVGL), Dec 2016 (hosted by Prof. Silvio Savarese).</li>
		<li> Adobe Research, Dec 2016 (hosted by Dr. Joon-Young Lee).</li>
		<li> <a href="http://wwwhome.ewi.utwente.nl/~truongkp/icmi2016-assp4mi/">ASSP4MI</a> workshop of <a href="https://icmi.acm.org/2016/index.php?id=program">ICMI</a>, Nov 2016. 
		<li> CMU, <a href="http://www.cs.cmu.edu/~learning/">Machine Learning Lunch Seminar</a>, Oct 2016.</li>
		<li> ICCV Oral Talk, Dec 2015 <a href='http://videolectures.net/iccv2015_joo_panoptic_studio/'>(video link)</a>.</li>
		<li> CMU, <a href="http://www.ri.cmu.edu/event_detail.html?event_id=1151&&menu_id=422&event_type=seminars">VASC Seminar</a>, Dec 2015.</li>
		<li> ETH Zurich, Computer Vision and Geometry lab, Oct 2015 (hosted by Prof. Marc Pollefeys).</li>
		<li> Seoul National University, June 2015 (hosted by Prof. Kyoung Mu Lee).</li>
		<li> ETRI, CG Team, May 2015 (hosted by Dr. Seong-Jae Lim).</li>
		<li> KAIST, May 2015 (hosted by Prof. In So Kweon).</li>
	</ul>

<li> <p> <b>"The Panoptic Studio (CVPR14)"</b>
	<ul>
		<li> Graduate Seminar, Civil &amp; Environmental Engineering, CMU, Feb 2015 (hosted by Prof. Hae Young Noh).</li>
		<li> People Image Analysis Consortium, CMU, Nov 2014.</li>
		<li> Reality Computing Meetup, Autodesk, Nov 2014.</li>
	</ul>
<li> <p> <b>"MAP Visibility Estimation for Large-Scale Dynamic 3D Reconstruction"</b></li>
	<ul>
		<li> CVPR Oral Talk, June 2014</li>
		<li> CMU, <a href="http://www.ri.cmu.edu/event_detail.html?event_id=902&&menu_id=427&event_type=seminars">VASC Seminar</a>, June 2014.</li>

	</ul>

</ul >


<h2>Press Coverage</h2>
<ul>

<p>

<table>
	<tr align="center">
	<td> 
			<img src="/img/wired_han_crop2.jpg" border=0 style="width: 300px;"></a> <br>
			WIRED, 2018 <br>[<a href="https://www.wired.com/story/inside-the-panoptic-studio/" target="_blank">Video + Article</a>]
		</td>
	
		<td> 
			<img src="/img/reuters_2017.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> <br>
			Reuters, 2017 <br>[<a href="https://www.reuters.com/video/2017/09/22/500-camera-dome-trains-computer-to-read?videoId=372591598" target="_blank">Video</a>]
		</td>
		<td>
			<img src="/img/verge_2.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> <br>
			The Verge, 2016 <br>[<a href="https://youtu.be/Ryawcs6OZmI?t=1m7s" target="_blank">Video</a>, <a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox" target="_blank">Article</a>]

		</td>
		
	</tr>
	<tr align="center">
	<td>
			<img src="/img/reuters.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a>  <br>
			Reuters, 2015 <br> [<a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429" target="_blank">Video + Article</a>]
		 </td>
		<td>
			<img src="/img/discovery.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> <br>
			Daily Planet, Discovery, 2015 <br><a href="https://youtu.be/GcX-mwltlgk?t=1m27s" target="_blank">[Video]</a>
		</td>
		<td>
			<img src="/img/ebs1.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a>  <br>
			Docuprime, EBS (Korean TV), 2017 <br>[<a href="https://www.youtube.com/watch?v=Axe_VkhBjPw&feature=youtu.be&t=3m20s" target="_blank">Video</a>]
		</td>
	</tr>
</table>

<li> IEEE Spectrum, <a href="https://spectrum.ieee.org/video/robotics/robotics-software/robots-learn-to-speak-body-language"> Robots Learn to Speak Body Language</a>, 2017

<li> ZDNet, <a href="http://www.zdnet.com/article/keypoint-detection-unlocks-secrets-of-body-language/">CMU researchers create a huge dome that can read body language</a>, 2017

<li> TechCrunch, <a href="https://techcrunch.com/2017/07/07/cmu-researchers-create-a-huge-dome-that-can-read-body-language/">CMU researchers create a huge dome that can read body language</a>, 2017

<li> BBC News, <a href="http://www.bbc.com/news/av/technology-41584732/the-dome-which-could-help-machines-understand-behaviour">The dome which could help machines understand behaviour</a>, 2017
</li>

<li> CMU News, <a href="http://www.cmu.edu/news/stories/archives/2017/march/robotics-tepper-negotiation-study.html">Scientists put human interaction under the microscope</a>, 2017
</li>

<!--li> <a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox">Cracking The Elaborate Code</a>, The Verge, 2016 (with <a href="https://www.youtube.com/watch?v=Ryawcs6OZmI">Video</a>)
</li-->


<li> SPIEGEL ONLINE(German), <a href="http://www.spiegel.de/netzwelt/gadgets/das-panoptische-studio-computer-entziffern-koerpersprache-a-1068155.html">The panoptic Studio: Computer decipher the secrets of body language</a>, 2015 
</li>

<li> Co.DESIGN, <a href="http://www.fastcodesign.com/3054561/inside-a-robot-eyeball-science-will-decode-our-body-language">Inside A Robot Eyeball, Science Will Decode Our Body Language</a>, 2015
</li>

<li> WIRED (Italian) <a href="http://www.wired.it/tv/guarda-riprese-360-gradi-panoptic-studio">Panoptic Studio: The Latest Generation of Motion Capture</a>, 2015  </li>


<!--li> <a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429">Motion capture on a whole new level</a>, Reuters, 2015 (Video)  </li-->

<li> Voice of America, <a href="http://www.voanews.com/content/new-studio-yields-most-detailed-motion-capture-in-3d/2743347.html">New Studio Yields Most Detailed Motion Capture in 3D</a>, 2015 (Video) <br>  </li>


<!-- li> <a href="http://www.discovery.ca/dailyplanet"> Future Tech: Panoptic Studio</a>, Daily Planet, Discovery Channel Canada, 2015 <br> (TV show, in the linked website, click "Segments" and select "Future tech: Panoptic Studio")--> </li>


<!--li> <a href="http://www.slate.com/articles/video/video/2014/07/panoptic_studio_carnegie_mellon_s_3_d_camera_dome_can_capture_action.html">Freezing Memories in Time: Carnegie Mellon's amazing 3-D "Panoptic Studio"</a>, Slate, 2014  (Video)

<iframe width="560" height="315" src="//www.youtube.com/embed/kEYoiSPGPqA?list=UUYC4ijpFZY_CtdElWFyy-Gg?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe  </li -->


<li> CNet, <a href="http://www.cnet.com/videos/tomorrow-daily-021-new-video-capture-tech-the-rickroll-rickmote-episode-vii-new-x-wing/">Tomorrow Daily: New video capture tech, the Rickroll 'Rickmote,' a new X-wing, and more</a>, 2014 (Video)
<!-- iframe width="560" height="315" src="//www.youtube.com/embed/tpIfotFW-P4?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe --></li>
<li> NBCNews, <a href="http://www.nbcnews.com/tech/innovation/camera-studded-dome-tracks-your-every-move-precision-n161541"> Camera-Studded Dome Tracks Your Every Move With Precision</a>, 2014 </li>
<li> IEEE Spectrum, <a href="http://spectrum.ieee.org/tech-talk/computing/software/camerafilled-dome-recreates-full-3d-motion-scenes"> Camera-Filled Dome Recreates Full 3-D Motion Scenes</a>, 2014</li>
<li> Discovery News, <a href="http://news.discovery.com/tech/gear-and-gadgets/dome-of-500-cameras-creates-amazing-3-d-flicks-140724.htm"> Amazing 3-D Flicks from Dome of 500 Cameras?</a>, 2014</li>
<li> Gizmodo, <a href="http://gizmodo.com/a-dome-packed-with-480-cameras-captures-detailed-3d-ima-1608263411">A Dome Packed With 480 Cameras Captures Detailed 3D Images In Motion</a>,  2014 </li>
<li> The Verge, <a href="http://www.theverge.com/2014/7/21/5923767/panoptic-studio-3d-motion-capture-carnegie-melon">Scientists build a real Panopticon that captures your every move in 3D</a>, 2014 </li>
<li> ScienceDaily, <a href="http://www.sciencedaily.com/releases/2014/07/140717124950.htm">Hundreds of videos used to reconstruct 3-D motion without markers</a>, 2014</li>
<li> Phys.org, <a href="http://phys.org/news/2014-07-combinehundreds-videos-reconstruct-3d-motion.html">Researchers combine hundreds of videos to reconstruct 3D motion without markers</a>, 2014 </li>
<li> Engadget, <a href="http://www.engadget.com/2014/07/21/carnegie-motion-capture-dome/">Watch a dome full of cameras capture 3D motion in extreme detail</a>,  2014 </li>
<li> PetaPixel, <a href="http://petapixel.com/2014/07/21/researchers-use-480-camera-setup-accurately-capture-3d-motion/">Researchers Use a 480-Camera Dome to More Accurately Capture 3D Motion</a>  2014 </li>
<li> Gizmag, <a href="http://www.gizmag.com/3d-motion-reconstruction-camera-panoptic-dome/33033/">Camera-studded dome used to reconstruct 3D motion</a>, 2014</li>
<li> The Register, <a href="http://www.theregister.co.uk/2014/07/22/boffins_fill_a_dome_with_480_cameras_for_3d_motion_capture/">Boffins fill a dome with 480 cameras for 3D motion capture</a>  2014  </li>
<li> The Engineer, <a href="http://www.theengineer.co.uk/electronics/news/3d-motion-captured-without-markers/1018947.article">3D motion captured without markers</a>, 2014 </li>
<li> Popular Photography, <a href="http://www.popphoto.com/news/2014/07/carnegie-mellon-packs-480-cameras-dome-to-perfectly-track-3d-motion">Carnegie Mellon Packs 480 Cameras In A Dome To Perfectly Track 3D Motion</a>,  2014 </li>
<li> CMU News, <a href="http://www.cmu.edu/news/stories/archives/2014/july/july17_reconstructing3Dmotion.html">Carnegie Mellon Combines Hundreds of Videos To Reconstruct 3D Motion Without Use of Markers</a>, 2014 </li>
</p></ul>

<h2>Patents</h2>
<ul>
<li> <p><b>Motion capture apparatus and method (Patent No.: US 8805024 B2)</b><br>
<b> Hanbyul Joo</b>, Seong-Jae Lim, Ji-Hyung Lee, Bon-Ki Koo </li>
<li> <p><b>Method for automatic rigging and shape surface transfer of 3D standard mesh model based on muscle and nurbs by using parametric control (Patent No.: US 7171060 B2)</b><br>
Seong Jae Lim, Ho Won Kim, <b>Hanbyul Joo</b>, Bon Ki Koo</li>
<li><p> <b>3D model shape transformation method and apparatus (Patent Application No.: US 20120162217 A1)</b><br> Seong-Jae Lim, <b>Hanbyul Joo</b>, Seung-Uk Yoon, Ji-Hyung Lee, Bon-Ki Koo. </li>
</ul>

</td> </table>
</body>
</html>

