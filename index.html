<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<title>Hanbyul Joo | Academic Website </title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:300,600' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="style.css">

	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50740543-1', 'cmu.edu');
  ga('send', 'pageview');

	</script>
</head>

<body>
<table border="0" cellpadding="15" cellspacing="0" width="1000">
<tr>
<td>&nbsp;</td>
</tr>

<tr>
<td valign="top">
</td>
	<td valign="top">
		<table border="0" cellpadding="0" cellspacing="0" width="100%">
			<td align="left" width="40%">
			<!--img width="60" src="img/RI_logo.jpg"-->
			<p>
			
			<p><span class="Name"><a id="Home"></a><b>Hanbyul Joo</b></span><br>
			(Also known as "Han") <br>
			Ph.D. <del>candidate</del> <br> <br>
		 	   <a href="http://www.ri.cmu.edu">The Robotics Institute</a><br> 
			    <a href="http://www.cs.cmu.edu">School of Computer Science</a><br>
			    <a href="http://www.cmu.edu">Carnegie Mellon University</a><br>

			    Office: EDSH 201<br>
			    Email:  hanbyulj at cs dot cmu dot edu <br><br>
			    <a href="CV_Hanbyul.pdf"> CV</a> (updated in Jun. 2018)
			 <p>
		</td>
		<td  valign="bottom" align="left">
			<table>
			<tr>	
			<!--td> <img src="img/hbjoo.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> </td-->
			<td> <img src="/img/han_dec_2017.jpg" border=0 style="max-height: 220px "></a> </td>
			<td> <img src="/img/Dome_withME_s.jpg" border=0 style="max-height: 220px"></a> </td>
			</tr>
			</table>
		</td>
		</table>

<p>
<hr size="1" align="left" noshade>
<p>
I finished my Ph.D. in the Robotics Institute at Carnegie Mellon University, under the supervision of Prof. <a href="http://www.cs.cmu.edu/~yaser/">Yaser Sheikh</a>. I interned at Facebook Reality Labs, Pittsburgh (Summer and Fall, 2017) and Disney Research Zurich (Summer, 2015). Before joining CMU, I spent three years as a researcher at ETRI, a government-funded research organization in Korea. I received my M.S. in Electrical Engineering (under the supervision of Prof. In So Kweon), and B.S. in Computer Science, both
from KAIST, Korea. <a href="http://slsf.or.kr/community/HomeEn.screen"> The Samsung Scholarship </a> partly supported my graduate study, and I am a recipient of the <a href="http://cvpr2018.thecvf.com/program/main_conference#awards">CVPR Best Student Paper Award</a> in 2018. </p>

<h2>Research</h2>
<!--p>
My main research interests are in computer vision, computer graphics, and machine perception of social behavior. My work focuses on analyzing and understanding dynamic scenes using more than 500 synchronized cameras in the Panoptic Studio at CMU.
</p-->
<p>
The goal of my research is the creation of a new scientific discipline of <b>computational behavioral science</b>, by <b>measuring the full spectrum of social signals</b> transmitted during an interpersonal social interaction. I have been developing a unique sensing system at CMU, the Panoptic Studio, which is composed of more than 500 synchronized cameras. My research is based on the tools of <b>computer vision, machine learning, computer graphics, and robotics</b>. 
</p>

<p align="center">

<table width="200" border="0" >
            <tbody>

              <tr>
				  <td><iframe width="360" height="202" src="https://youtube.com/embed/5QzdXQSf-oY?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></td>
                 <td>&nbsp;</td>
				  <td><iframe width="360" height="202" src="https://www.youtube.com/embed/wb32z_xwk0c?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></td>
                <td>&nbsp;</td>
				  <td><iframe width="360" height="202" src="https://www.youtube.com/embed/d_xdrdN1Xw8?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></td>
              
			  </tr>
              <tr align="center">
                <td><a href="/totalcapture">Markerless Total Motion Capture<br> </td>
                <td>&nbsp;</td>
                <td><a href="http://domedb.perception.cs.cmu.edu/">The Panoptic Studio <br> </td>
				<td>&nbsp;</td>
                <td><a href="http://domedb.perception.cs.cmu.edu/">Panoptic DB (2D/3D body keypoint annotations) <br> </td>
              </tr>
            </tbody>
          </table>
		  
<!--iframe width="360" height="202" src="https://www.youtube.com/embed/Ryawcs6OZmI" frameborder="0" allowfullscreen></iframe-->
</p>

<!--p align="center">
<iframe width="560" height="315" src="//www.youtube.com/embed/xoPQnmkufeM?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
<h4 align="center"> Virtural Hanbyul in the Panoptic Studio</h4>
</p-->


<h2>News</h2>

<ul>
<li> <p> <b> Dec 2018</b> I have defended my PhD thesis!! I will be joining Facebook AI Research as a Research Scientist from Jan 2019. </p></li>


<li> <p> <b> Sep 2018</b> I talked about our research direction in the interview with <a href="https://twimlai.com/twiml-talk-180-can-we-train-an-ai-to-understand-body-language-with-hanbyul-joo/">"This Week in Machine Learning & AI"</a></p></li>

<li> <p> <b> Jun 2018</b> Our <a href="/totalcapture/">Total Capture paper</a> received <a href="http://cvpr2018.thecvf.com/program/main_conference#awards"><b>Best Student Paper Award</b> at CVPR 2018 <a></p></li>
<iframe width="360" height="202" src="https://www.youtube.com/embed/gz2VoDrvX-A?rel=0&amp;showinfo=0&amp;start=1579" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<a href="https://youtu.be/gz2VoDrvX-A?t=26m19s" target="_blank"><img style="max-height: 202px;" src="/img/cvpr_award.jpg" border="0"></a> </p></li>

<li> <p> <b> Feb 2018</b> Our research has been featured on <a href="https://www.wired.com/story/inside-the-panoptic-studio/">WIRED</a><!--: <br> &nbsp;&nbsp;
<a href="https://www.wired.com/story/inside-the-panoptic-studio/"><img style="max-height: 200px;" src="img/wired_han.jpg" border="0"></a> </p></li--->

<li> <p> <b> Feb 2018</b> I have two papers accepted to CVPR 2018 </p></li>


<li> <p> <b> Oct 2017</b> Our research has been featured on <a href="https://www.reuters.com/video/2017/09/22/500-camera-dome-trains-computer-to-read?videoId=372591598">Reuters</a> and <a href="http://www.bbc.com/news/av/technology-41584732/the-dome-which-could-help-machines-understand-behaviour">BBC News</a>
</p></li>

<li> <p> <b> Jul 2017</b>
Our <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose Library</a> and <a href="http://domedb.perception.cs.cmu.edu">Panoptic Studio</a> have been covered in multiple media outlets: <a href="http://spectrum.ieee.org/video/robotics/robotics-software/robots-learn-to-speak-body-language">IEEE Spectrum</a>, <a href="https://techcrunch.com/2017/07/07/cmu-researchers-create-a-huge-dome-that-can-read-body-language/">TechCrunch</a>, <a href="http://www.zdnet.com/article/keypoint-detection-unlocks-secrets-of-body-language/">ZDNet</a>, <a href="http://triblive.com/business/technology/12522553-74/at-cmus-panoptic-studio-531-cameras-capture-the-tiniest-details-of-human">Tribune-Review</a>, <a href="https://www.sciencedaily.com/releases/2017/07/170706143158.htm">ScienceDaily</a>, <a href="https://www.cs.cmu.edu/news/computer-reads-body-language">CMU News</a>, and so on. 

</p></li>

<li> <p> <b> Jul 2017</b> I am co-organizing a tutorial on <a href="http://domedb.perception.cs.cmu.edu/tutorials/cvpr17">"DIY: A Multiview Camera System: Panoptic Studio Teardown"</a> in conjunction with CVPR 2017
</p></li>

<li> <p> <b> Jul 2017</b> The code and realtime demo of our hand paper has been released as a part of <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">CMU OpenPose Library</a>
</p></li>




<li> <p> <b> May 2017</b> Since May 2017, I am interning at Oculus Research Pittsburgh.</p></li>

<li> <p> <b> May 2017</b> I have successfully finished my thesis proposal: <a href="http://ri.cmu.edu/event/hanbyul-joo-measuring-and-modeling-kinesic-signals-in-social-communication/">Thesis Info</a>
</p></li>

<li> <p> <b> April 2017</b> I was on a Korean TV channel (EBS), introducing our research: <a href="https://youtu.be/Axe_VkhBjPw?t=3m20s">Video</a>
</p></li>

<!--li> <p> <b> Dec 2016</b> I gave talks in Bay Area (Stanford, UC Berkeley, and Adobe Research) to present our recent progress in social interaction capture using our Panoptic Studio. 
</p></li>


<li> <p> <b> Dec 2016</b> Our recent progress on the Panoptic Studio is featured on <a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox"><img src="/logos/Verge.png" height="20"></a>. You can also see the video version <a href="https://www.youtube.com/watch?v=Ryawcs6OZmI">here</a>. 
</p></li>


<li> <p> <b> Nov 2016</b> I gave a talk about our Panoptic Studio system in <a href="http://wwwhome.ewi.utwente.nl/~truongkp/icmi2016-assp4mi/">ASSP4MI</a> workshop of <a href="https://icmi.acm.org/2016/index.php?id=program">ICMI</a>. 
</p></li>


<li> <p> <b> Sep 2016</b> Our brand new <b>Panoptic Studio dataset website</b> is open <a href="http://domedb.perception.cs.cmu.edu">(link)</a>. All dataset including videos from 500+ cameras and 3D skeltons reconstruction results, and a Toolbox are available. 
</p></li>

<li> <p> <b> Dec 2015</b> Our Social Motion Capture paper has been covered in the press: <a href="http://www.spiegel.de/netzwelt/gadgets/das-panoptische-studio-computer-entziffern-koerpersprache-a-1068155.html">SPIEGEL ONLINE (German)</a> and <a href="http://www.fastcodesign.com/3054561/inside-a-robot-eyeball-science-will-decode-our-body-language">Co.DESIGN</a>.
</p></li>


<li> <p> <b> Jun-Oct 2015</b> I have finished my internship at Disney Research Zurich, and came back to CMU. 
</p></li>

<li> <p> <b> Sep 2015</b> Our "Social Motion Capture" paper has been accepted as an oral presentation in ICCV 2015.
</p></li>

<li> <p> <b> Apr 2015</b> Our massively multiple view system and dynamic 3D reconstruction has been featured on the <a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429">
<img src="http://www.cs.cmu.edu/~hanbyulj/logos/Reuters_logo.png" height="20"></div></a> and <a href="http://www.voanews.com/content/new-studio-yields-most-detailed-motion-capture-in-3d/2743347.html"><img src="http://www.cs.cmu.edu/~hanbyulj/logos/VOA_logo.png" height="17"></a>. Click the links to see the articles.

</p></li>

<li> <p> <b> Jan 2015</b> Our massively multiple view system and dynamic 3D reconstruction has been featured on <img src="http://img4.wikia.nocookie.net/__cb20091118171916/logopedia/images/4/49/Discovery_Channel_2009.png" height="17">, Daily Planet TV Show. You can see the segment in <a href="http://www.discovery.ca/dailyplanet"> Daily Planet website</a>. 


<li><p><b> Aug 2014</b> The dataset of our <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibility.html"> dynamic 3D reconstruction paper </a> is now available in <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibilityDataset.html">this webpage</a>.</p></li>

<li><p><b> July 2014</b> Our <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibility.html"> dynamic 3D reconstruction paper </a> is featured on

<a href="http://spectrum.ieee.org/tech-talk/computing/software/camerafilled-dome-recreates-full-3d-motion-scenes"><img src="/logos/IEEESpectrum.jpeg" height="17"></a>, 
 <a href="http://www.nbcnews.com/tech/innovation/camera-studded-dome-tracks-your-every-move-precision-n161541"><img src="/logos/nbc-news.png" height="15"></a>, 
<a href="http://news.discovery.com/tech/gear-and-gadgets/dome-of-500-cameras-creates-amazing-3-d-flicks-140724.htm"><img src="/logos/discovery.jpg" height="17"></a>, and so on. Check out the <a href="http://www.cs.cmu.edu/~hanbyulj/14/visibility.html">project page</a>. </p> </li-->
</ul>


<h2>Dataset/Library</h2>
<table cellspacing="15">
<tr>
	<td width="30%">
	<a href="http://domedb.perception.cs.cmu.edu"><img style="max-height: 200px; max-width: 250px;" src="/img/ExampleResults.jpg" border="0"></a>
	</td>
	<td>
 		<p> <a href="http://domedb.perception.cs.cmu.edu">CMU Panoptic Studio Dataset</a>
	</td>
</tr>
<tr>
	<td width="30%">
	<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose"><img style="max-height: 200px; max-width: 250px;" src="/img/pose_face_hands.gif" border="0"></a>
	</td>
	<td>
 		<p> <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose Library</a>
	</td>
</tr>
</table>


<h2>Tutorial</h2>
<table cellspacing="15">
<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="http://domedb.perception.cs.cmu.edu/tutorials/cvpr17/fig/summary.jpg" border="0">
	</td>
	<td>
	<p> <b> DIY A Multiview Camera System: Panoptic Studio Teardown </b>
	<br>
	<b>Hanbyul Joo</b>, Tomas Simon, Hyun Soo Park, Shohei Nobuhara, and Yaser Sheikh
	<br>
	In Conjunction with <em>CVPR 2017 </em>  <br>
			<a href="http://domedb.perception.cs.cmu.edu/tutorials/cvpr17/index.html">[Tutorial Page]</a> <a href="https://www.youtube.com/watch?v=wmQF3XKvL8s">[Video]</a>
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 


</table>

<h2>Publications</h2>
<table cellspacing="15">


<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="/img/totalcapture.gif" border="0">
	</td>

	<td>
	<p> <b> Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies </b>
	<br>
	<b>Hanbyul Joo</b>, Tomas Simon, and Yaser Sheikh
	<br>
	In <em> CVPR 2018</em> &nbsp  <b> (Oral) </b> <font class="ratio"> - Acceptance ratio: 70/3303~2.1% </font> <br>
	<a href="http://cvpr2018.thecvf.com/program/main_conference#awards"><font color="blue"><b>[CVPR Best Student Paper Award]</b></font></a> <br>
	<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Joo_Total_Capture_A_CVPR_2018_paper.pdf ">[PDF]</a>
	<a href="/totalcapture/totalBody_camready_supp.pdf">[Supplementary Material]</a>
	<a href="https://www.youtube.com/watch?v=5QzdXQSf-oY">[video]</a>
	<a href="/totalcapture">[Project Page]</a>
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 


<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="/img/cvpr18_skate.jpg" border="0">
	</td>

	<td>
	<p> <b> Structure from Recurrent Motion: From Rigidity to Recurrency</b>
	<br>Xiu Li, Hongdong Li, <b>Hanbyul Joo</b>, Yebin Liu, Yaser Sheikh 
	<br>
	In <em> CVPR 2018</em> <br>
	<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Structure_From_Recurrent_CVPR_2018_paper.pdf">[pdf]</a>
	<!--a href="https://arxiv.org/abs/1801.01615">[arXiv]</a>
	<a href="https://www.youtube.com/watch?v=5QzdXQSf-oY">[video]</a>
	<a href="http://www.cs.cmu.edu/~hanbyulj/totalcapture">[Project Page]</a-->
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 



<tr>
	<td width="30%">
	<img style="max-height: 300px; max-width: 250px;" src="/img/teaser_mvbs.jpg" border="0">
	</td>
	<td>
	<p> <b> Hand Keypoint Detection in Single Images using Multiview Bootstrapping </b>
	<br>
	<a href="http://www.cs.cmu.edu/~tsimon">Tomas Simon</a>, <b>Hanbyul Joo</b>, Iain Mattews, and Yaser Sheikh
	<br>
	In <em>CVPR 2017 </em>  <br>
	<a href="http://arxiv.org/abs/1704.07809">[arXiv]</a> <a href="http://www.cs.cmu.edu/~tsimon/projects/mvbs.html">[Project Page]</a>
	<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">[Code]</a>
	<a href="http://domedb.perception.cs.cmu.edu/handdb.html">[Dataset]</a>
						 <!-- | <a href="youtube-link">video</a><h3> </h3> -->
						<!-- | code (coming soon) -->
	</td>
</tr> 


<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/panoptic.jpg" border="0">
	</td>
	<td>
 		<p> <b>  Panoptic Studio: A Massively Multiview System for Social Interaction Capture </b>
	<br>
	<b>Hanbyul Joo</b>, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh
	<br>
	In <em> TPAMI 2017 (Extended version of ICCV15)</em>  <br>

	<a href="http://ieeexplore.ieee.org/document/8187699">[published version]</a>
	<a href="https://arxiv.org/abs/1612.03153">[arXiv version]</a>
	<a href="http://domedb.perception.cs.cmu.edu">[Dataset]</a> <br>
	</td>
</tr> 

<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/007_bang.gif" border="0">
	</td>
	<td>
 		<p> <b>  Panoptic Studio: A Massively Multiview System for Social Motion Capture </b>
	<br>
	<b>Hanbyul Joo</b>, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara,
	and Yaser Sheikh
	<br>
	In <em>ICCV 2015</em> &nbsp <b> (Oral) </b> <font class="ratio"> - Acceptance ratio: 56/1698~3.3% </font>
 
	<br>

	<a href="/panoptic-studio/ICCV2015_SMC.pdf"> [Paper(PDF)]</a> 
	<a href="/panoptic-studio/ICCV2015_SMC_Supp.pdf"> [Supplementary Material]</a> 

	<a href="/panoptic-studio">[Project Page]</a>  <br>
	Press Coverage: 


<a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox"><img src="/logos/Verge.png" height="20"></a>
<a href="https://www.technologyreview.com/s/601276/5-things-you-need-to-know-about-facebooks-next-10-years/"><img src="https://d267cvn3rvuq91.cloudfront.net/_/img/stacked-logo.svg?v=2" height="30"></a>
<!--a href="http://www.fastcodesign.com/3054561/inside-a-robot-eyeball-science-will-decode-our-body-language"><img src="https://pbs.twimg.com/profile_images/487710482539630592/ZuFzG-w6.png" height="40"></a-->
<a href="http://www.spiegel.de/netzwelt/gadgets/das-panoptische-studio-computer-entziffern-koerpersprache-a-1068155.html"><img src="/logos/spiegelOnline.jpg" height="15"></a>


</p>
	</td>
</tr>  


<tr>
	<td width="30%">
	<!--img style="max-height: 200px; max-width: 250px;" src="img/CVPR14.jpg" border="0" -->
	<img style="max-height: 200px; max-width: 250px;" src="/img/confetti.gif" border="0">
	</td>
	<td>
 <p> <b>MAP Visibility Estimation for Large-Scale Dynamic 3D Reconstruction</b>
	<br>
	<b>Hanbyul Joo</b>, Hyun Soo Park, and Yaser Sheikh
	<br>
	In <em>CVPR 2014</em> &nbsp <b>(Oral)</b> <font class="ratio"> - Acceptance ratio: 104/1807~5.8% </font>
	<br>

	<a href="/14/CVPR_2014_Visibility.pdf"> [Paper(PDF)]</a> 
	<a href="/14/visibility.html"> [Project Page]</a> <a href="/14/visibilityDataset.html">[Dataset]</a> <br>
	Press Coverage: 
<a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429"><img src="/logos/Reuters_logo.png" height="20"></div></a>
<a href="http://www.voanews.com/content/new-studio-yields-most-detailed-motion-capture-in-3d/2743347.html"><img src="/logos/VOA_logo.png" height="17"></a>
<a href="http://spectrum.ieee.org/tech-talk/computing/software/camerafilled-dome-recreates-full-3d-motion-scenes"><img src="/logos/IEEESpectrum.jpeg" height="20"></a>
<a href="http://www.cnet.com/videos/tomorrow-daily-021-new-video-capture-tech-the-rickroll-rickmote-episode-vii-new-x-wing/"><img src="http://upload.wikimedia.org/wikipedia/en/8/8f/Cnetlogo.png" height="30"></a>
<a href="http://www.nbcnews.com/tech/innovation/camera-studded-dome-tracks-your-every-move-precision-n161541"><img src="/logos/nbc-news.png" height="15"></a>
<a href="http://news.discovery.com/tech/gear-and-gadgets/dome-of-500-cameras-creates-amazing-3-d-flicks-140724.htm"><img src="/logos/discovery.jpg" height="20"></a>
<a href="http://www.engadget.com/2014/07/21/carnegie-motion-capture-dome/"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Engadget-logo.svg/200px-Engadget-logo.svg.png" height="20"></a>
<a href="http://gizmodo.com/a-dome-packed-with-480-cameras-captures-detailed-3d-ima-1608263411"><img src="http://i.kinja-img.com/gawker-media/image/upload/s--NkOZEtU4--/xxtpkkwu6xvbcaoxjbx2.png" height="13"><a>
<a href="http://www.theverge.com/2014/7/21/5923767/panoptic-studio-3d-motion-capture-carnegie-melon"><img src="/logos/Verge.png" height="20"></a>
<a href="http://www.sciencedaily.com/releases/2014/07/140717124950.htm"><img src="https://www.sciencedaily.com/images/sd-logo.png" height="15"></a>
<a href="http://phys.org/news/2014-07-combinehundreds-videos-reconstruct-3d-motion.html"><img src="/logos/phyOrg.jpg" height="25"></a>
<a href="http://www.slate.com/articles/video/video/2014/07/panoptic_studio_carnegie_mellon_s_3_d_camera_dome_can_capture_action.html"><img src="http://upload.wikimedia.org/wikipedia/commons/1/13/Slate_logo.png" height="22"></a>
<a href="http://petapixel.com/2014/07/21/researchers-use-480-camera-setup-accurately-capture-3d-motion/"><img src="/logos/petapixel.jpg" height="25"></a>
<a href="http://www.gizmag.com/3d-motion-reconstruction-camera-panoptic-dome/33033/"><img src="/logos/gizmag.jpeg" height="25"></a>
<a href="http://www.theregister.co.uk/2014/07/22/boffins_fill_a_dome_with_480_cameras_for_3d_motion_capture/"><img src="/logos/TheRegisterLogo.jpg" height="20"></a>
<a href="http://www.theengineer.co.uk/electronics/news/3d-motion-captured-without-markers/1018947.article"><img src="/logos/theEngineer.png" height="18"></a>...

<!--a href="http://www.popphoto.com/news/2014/07/carnegie-mellon-packs-480-cameras-dome-to-perfectly-track-3d-motion"><img src="/logos/PopPhoto.jpg" height="20"></a>
<a href="http://www.cmu.edu/news/stories/archives/2014/july/july17_reconstructing3Dmotion.html"><img src="/logos/CMU.jpg" height="40"></a> 
<a href="http://www.eurekalert.org/pub_releases/2014-07/cmu-cmc071714.php"><img src="/logos/EurekAlert.jpg" height="22"></a>

<a href="http://www.azorobotics.com/news.aspx?newsID=5985"><img src="/logos/AzoRobot.jpg" height="20"></a>

<a href="http://www.slrlounge.com/latest-3d-tech-geodome-captures-every-move-480-cameras"><img src="/logos/slrLounge.jpg" height="20"></a>

<a href="http://security-today.com/articles/2014/07/23/panopticon-captures-every-movement-in-3d.aspx"><img src="/logos/securityToday.jpg" height="20"></a-->

	</td>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/icip11_2.jpg" border="0">
	</td>
	<td>
	<p><b>Graph-based Shape Matching for Deformable Objects</b>
	<br>
	<b>Hanbyul Joo</b>,
	Yekeun Jeong, Olivier Duchenne, and In So Kweon
	<br>
	In <em> ICIP 2011</em> 
	<br>
		<a href="/papers/ICIP2011_JOO_FINAL.pdf">
	[Paper(PDF)]
	</a> </p>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/icra09.jpg" border="0">
	</td>
	<td>
	<p><b>Graph-Based Robust Shape Matching for Robotic Application</b>
	<br>
	<b>Hanbyul Joo</b>,
	Yekeun Jeong, Olivier Duchenne, Seong-Young Ko, and In So Kweon
	<br>
	In <em>ICRA 2009</em> 
	<br>
	<a href="/papers/ICRA2009.pdf">
	[Paper(PDF)]
	</a> </p>
	</td>
</tr>

<tr>
	<td width="30%">
	<img style="max-height: 200px; max-width: 250px;" src="/img/MVA07.jpg" border="0">
	</td>
	<td>
	<p><b>Statistical Background Subtraction Based on the Exact Per-pixel Distributions</b>
	<br>
	Youngbae Hwang, <b>Hanbyul Joo</b>, Jun-sik Kim, and In So Kweon
	<br>
	In <em>IAPR workshop on Machine Vision Applications (MVA) 2007</em> 
	<br>
	<a href="
http://rcv.kaist.ac.kr/~unicorn/paper/hwang_MVA_2007.pdf">
	[Paper(PDF)]
	</a></p>
	</td>
</tr>


</table>
 
<!--img 
src="http://monster.gostats.com/bin/count?a=400436&amp;t=4&amp;i=19&amp;z=" 
style="border-width:0px" alt="Free web counters" /><br / 
<small>since Jan 2007. </small -->
<!-- End GoStats.com Simple HTML based code -->
<h2>Talks</h2>
<ul>
<li> <p> <b>"Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies"</b></p></li>
	<ul>
		<li> <a href="https://games-cn.org/games-webinar-20181025-70/">GAMES Webinar</a>, Oct 2018 (hosted by Prof. Yebin Liu).</li>
		<li> CVPR Oral Talk, June 2018 <a href="https://youtu.be/XWr0Fg5XbPs?t=268">(video link)</a>.</li>
	</ul>
<li> <p> <b>"Measuring and Modeling Social Signals for Computational Behavioral Understanding"</b></p></li>
	<ul>
		<li> UC Berkeley, BAIR, MAY 2018 (hosted by Prof. Jitendra Malik).</li>
		<li> UT Austin, School of Computer Science, April 2018.</li>
		<li> CMU, School of Computer Science, April 2018.</li>
		<li> MIT, CSAIL, April 2018.</li>
		<li> MIT, Media Lab, Nov 2017.</li>
	</ul>
<li> <p> <b>"The Panoptic Studio: A Massively Multiview System for Social Interaction Capture"</b></p></li>
	<ul>

		<li> UC Berkeley, Computer Vision Group, Dec 2016 (hosted by Prof. Alexei A. Efros).</li>
		<li> Stanford, Computer Vision and Geometry Lab (CVGL), Dec 2016 (hosted by Prof. Silvio Savarese).</li>
		<li> Adobe Research, Dec 2016 (hosted by Dr. Joon-Young Lee).</li>
		<li> <a href="http://wwwhome.ewi.utwente.nl/~truongkp/icmi2016-assp4mi/">ASSP4MI</a> workshop of <a href="https://icmi.acm.org/2016/index.php?id=program">ICMI</a>, Nov 2016. 
		<li> CMU, <a href="http://www.cs.cmu.edu/~learning/">Machine Learning Lunch Seminar</a>, Oct 2016.</li>
		<li> ICCV Oral Talk, Dec 2015 <a href='http://videolectures.net/iccv2015_joo_panoptic_studio/'>(video link)</a>.</li>
		<li> CMU, <a href="http://www.ri.cmu.edu/event_detail.html?event_id=1151&&menu_id=422&event_type=seminars">VASC Seminar</a>, Dec 2015.</li>
		<li> ETH Zurich, Computer Vision and Geometry lab, Oct 2015 (hosted by Prof. Marc Pollefeys).</li>
		<li> Seoul National University, June 2015 (hosted by Prof. Kyoung Mu Lee).</li>
		<li> ETRI, CG Team, May 2015 (hosted by Dr. Seong-Jae Lim).</li>
		<li> KAIST, May 2015 (hosted by Prof. In So Kweon).</li>
	</ul>

<li> <p> <b>"The Panoptic Studio (CVPR14)"</b>
	<ul>
		<li> Graduate Seminar, Civil &amp; Environmental Engineering, CMU, Feb 2015 (hosted by Prof. Hae Young Noh).</li>
		<li> People Image Analysis Consortium, CMU, Nov 2014.</li>
		<li> Reality Computing Meetup, Autodesk, Nov 2014.</li>
	</ul>
<li> <p> <b>"MAP Visibility Estimation for Large-Scale Dynamic 3D Reconstruction"</b></li>
	<ul>
		<li> CVPR Oral Talk, June 2014 <a href="http://techtalks.tv/talks/map-visibility-estimation-for-large-scale-dynamic-3d-reconstruction/60293">(video link)</a>.</li>
		<li> CMU, <a href="http://www.ri.cmu.edu/event_detail.html?event_id=902&&menu_id=427&event_type=seminars">VASC Seminar</a>, June 2014.</li>

	</ul>

</ul >


<h2>Press Coverage</h2>
<ul>

<p>

<table>
	<tr align="center">
	<td> 
			<img src="/img/wired_han_crop2.jpg" border=0 style="width: 300px;"></a> <br>
			WIRED, 2018 <br>[<a href="https://www.wired.com/story/inside-the-panoptic-studio/" target="_blank">Video + Article</a>]
		</td>
	
		<td> 
			<img src="/img/reuters_2017.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> <br>
			Reuters, 2017 <br>[<a href="https://www.reuters.com/video/2017/09/22/500-camera-dome-trains-computer-to-read?videoId=372591598" target="_blank">Video</a>]
		</td>
		<td>
			<img src="/img/verge_2.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> <br>
			The Verge, 2016 <br>[<a href="https://youtu.be/Ryawcs6OZmI?t=1m7s" target="_blank">Video</a>, <a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox" target="_blank">Article</a>]

		</td>
		
	</tr>
	<tr align="center">
	<td>
			<img src="/img/reuters.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a>  <br>
			Reuters, 2015 <br> [<a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429" target="_blank">Video + Article</a>]
		 </td>
		<td>
			<img src="/img/discovery.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a> <br>
			Daily Planet, Discovery, 2015 <br><a href="https://youtu.be/GcX-mwltlgk?t=1m27s" target="_blank">[Video]</a>
		</td>
		<td>
			<img src="/img/ebs1.jpg" border=0 style="max-height: 300px; max-width: 300px;"></a>  <br>
			Docuprime, EBS (Korean TV), 2017 <br>[<a href="https://www.youtube.com/watch?v=Axe_VkhBjPw&feature=youtu.be&t=3m20s" target="_blank">Video</a>]
		</td>
	</tr>
</table>

<li> IEEE Spectrum, <a href="https://spectrum.ieee.org/video/robotics/robotics-software/robots-learn-to-speak-body-language"> Robots Learn to Speak Body Language</a>, 2017

<li> ZDNet, <a href="http://www.zdnet.com/article/keypoint-detection-unlocks-secrets-of-body-language/">CMU researchers create a huge dome that can read body language</a>, 2017

<li> TechCrunch, <a href="https://techcrunch.com/2017/07/07/cmu-researchers-create-a-huge-dome-that-can-read-body-language/">CMU researchers create a huge dome that can read body language</a>, 2017

<li> BBC News, <a href="http://www.bbc.com/news/av/technology-41584732/the-dome-which-could-help-machines-understand-behaviour">The dome which could help machines understand behaviour</a>, 2017
</li>

<li> CMU News, <a href="http://www.cmu.edu/news/stories/archives/2017/march/robotics-tepper-negotiation-study.html">Scientists put human interaction under the microscope</a>, 2017
</li>

<!--li> <a href="http://www.theverge.com/2016/12/7/13857144/social-vr-carnegie-mellon-panoptic-studio-facebook-oculus-toybox">Cracking The Elaborate Code</a>, The Verge, 2016 (with <a href="https://www.youtube.com/watch?v=Ryawcs6OZmI">Video</a>)
</li-->


<li> SPIEGEL ONLINE(German), <a href="http://www.spiegel.de/netzwelt/gadgets/das-panoptische-studio-computer-entziffern-koerpersprache-a-1068155.html">The panoptic Studio: Computer decipher the secrets of body language</a>, 2015 
</li>

<li> Co.DESIGN, <a href="http://www.fastcodesign.com/3054561/inside-a-robot-eyeball-science-will-decode-our-body-language">Inside A Robot Eyeball, Science Will Decode Our Body Language</a>, 2015
</li>

<li> WIRED (Italian) <a href="http://www.wired.it/tv/guarda-riprese-360-gradi-panoptic-studio">Panoptic Studio: The Latest Generation of Motion Capture</a>, 2015  </li>


<!--li> <a href="http://www.reuters.com/article/2015/04/29/us-usa-panoptic-studio-idUSKBN0NK09920150429">Motion capture on a whole new level</a>, Reuters, 2015 (Video)  </li-->

<li> Voice of America, <a href="http://www.voanews.com/content/new-studio-yields-most-detailed-motion-capture-in-3d/2743347.html">New Studio Yields Most Detailed Motion Capture in 3D</a>, 2015 (Video) <br>  </li>


<!-- li> <a href="http://www.discovery.ca/dailyplanet"> Future Tech: Panoptic Studio</a>, Daily Planet, Discovery Channel Canada, 2015 <br> (TV show, in the linked website, click "Segments" and select "Future tech: Panoptic Studio")--> </li>


<!--li> <a href="http://www.slate.com/articles/video/video/2014/07/panoptic_studio_carnegie_mellon_s_3_d_camera_dome_can_capture_action.html">Freezing Memories in Time: Carnegie Mellon's amazing 3-D "Panoptic Studio"</a>, Slate, 2014  (Video)

<iframe width="560" height="315" src="//www.youtube.com/embed/kEYoiSPGPqA?list=UUYC4ijpFZY_CtdElWFyy-Gg?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe  </li -->


<li> CNet, <a href="http://www.cnet.com/videos/tomorrow-daily-021-new-video-capture-tech-the-rickroll-rickmote-episode-vii-new-x-wing/">Tomorrow Daily: New video capture tech, the Rickroll 'Rickmote,' a new X-wing, and more</a>, 2014 (Video)
<!-- iframe width="560" height="315" src="//www.youtube.com/embed/tpIfotFW-P4?rel=0;3&amp;autohide=1&amp;showinfo=0" frameborder="0" allowfullscreen></iframe --></li>
<li> NBCNews, <a href="http://www.nbcnews.com/tech/innovation/camera-studded-dome-tracks-your-every-move-precision-n161541"> Camera-Studded Dome Tracks Your Every Move With Precision</a>, 2014 </li>
<li> IEEE Spectrum, <a href="http://spectrum.ieee.org/tech-talk/computing/software/camerafilled-dome-recreates-full-3d-motion-scenes"> Camera-Filled Dome Recreates Full 3-D Motion Scenes</a>, 2014</li>
<li> Discovery News, <a href="http://news.discovery.com/tech/gear-and-gadgets/dome-of-500-cameras-creates-amazing-3-d-flicks-140724.htm"> Amazing 3-D Flicks from Dome of 500 Cameras?</a>, 2014</li>
<li> Gizmodo, <a href="http://gizmodo.com/a-dome-packed-with-480-cameras-captures-detailed-3d-ima-1608263411">A Dome Packed With 480 Cameras Captures Detailed 3D Images In Motion</a>,  2014 </li>
<li> The Verge, <a href="http://www.theverge.com/2014/7/21/5923767/panoptic-studio-3d-motion-capture-carnegie-melon">Scientists build a real Panopticon that captures your every move in 3D</a>, 2014 </li>
<li> ScienceDaily, <a href="http://www.sciencedaily.com/releases/2014/07/140717124950.htm">Hundreds of videos used to reconstruct 3-D motion without markers</a>, 2014</li>
<li> Phys.org, <a href="http://phys.org/news/2014-07-combinehundreds-videos-reconstruct-3d-motion.html">Researchers combine hundreds of videos to reconstruct 3D motion without markers</a>, 2014 </li>
<li> Engadget, <a href="http://www.engadget.com/2014/07/21/carnegie-motion-capture-dome/">Watch a dome full of cameras capture 3D motion in extreme detail</a>,  2014 </li>
<li> PetaPixel, <a href="http://petapixel.com/2014/07/21/researchers-use-480-camera-setup-accurately-capture-3d-motion/">Researchers Use a 480-Camera Dome to More Accurately Capture 3D Motion</a>  2014 </li>
<li> Gizmag, <a href="http://www.gizmag.com/3d-motion-reconstruction-camera-panoptic-dome/33033/">Camera-studded dome used to reconstruct 3D motion</a>, 2014</li>
<li> The Register, <a href="http://www.theregister.co.uk/2014/07/22/boffins_fill_a_dome_with_480_cameras_for_3d_motion_capture/">Boffins fill a dome with 480 cameras for 3D motion capture</a>  2014  </li>
<li> The Engineer, <a href="http://www.theengineer.co.uk/electronics/news/3d-motion-captured-without-markers/1018947.article">3D motion captured without markers</a>, 2014 </li>
<li> Popular Photography, <a href="http://www.popphoto.com/news/2014/07/carnegie-mellon-packs-480-cameras-dome-to-perfectly-track-3d-motion">Carnegie Mellon Packs 480 Cameras In A Dome To Perfectly Track 3D Motion</a>,  2014 </li>
<li> CMU News, <a href="http://www.cmu.edu/news/stories/archives/2014/july/july17_reconstructing3Dmotion.html">Carnegie Mellon Combines Hundreds of Videos To Reconstruct 3D Motion Without Use of Markers</a>, 2014 </li>
</p></ul>

<h2>Patents</h2>
<ul>
<li> <p><b>Motion capture apparatus and method (Patent No.: US 8805024 B2)</b><br>
<b> Hanbyul Joo</b>, Seong-Jae Lim, Ji-Hyung Lee, Bon-Ki Koo </li>
<li> <p><b>Method for automatic rigging and shape surface transfer of 3D standard mesh model based on muscle and nurbs by using parametric control (Patent No.: US 7171060 B2)</b><br>
Seong Jae Lim, Ho Won Kim, <b>Hanbyul Joo</b>, Bon Ki Koo</li>
<li><p> <b>3D model shape transformation method and apparatus (Patent Application No.: US 20120162217 A1)</b><br> Seong-Jae Lim, <b>Hanbyul Joo</b>, Seung-Uk Yoon, Ji-Hyung Lee, Bon-Ki Koo. </li>
</ul>

</td> </table>
</body>
</html>

